我们已经完成了机器级代码的所有课程

接下来......

我们已经知道了机器级代码，以及我们能用它来做什么

这节课是基于机器级代码的课程

现在你可以看懂机器级代码

这节课实际上覆盖了课本的第五章 优化程序性能

但我们只上一节课

并且，也没有课后实验（现在已经有第五章的lab了）

这种课程安排确实有点可惜

因为这是一个非常有趣的话题

一个我认为你应该掌握的知识点

这一章，考试中只会出一些小的题目

可以参考以前考试的题目

这一章的主题是，如果让程序运行更快

这里，运行的更快指的是算法已经确定了

并且程序可以正确运行

如何让它运行得更快

这是我们这一章的主题

你可以在不同的层面上做这件事情

比如，你可以先去掉程序中不必要的工作，这个优化不依赖与目标机器

这会让你的程序运行更快...

后面我会讲什么是编译器友好代码

要编写出编译器友好的代码，你必须明白什么代码编译器能够优化

什么代码编译器无法优化

并且你应该养成习惯

编写代码的时候，编写的代码都是编译器友好的代码

我们已经去掉了程序中不必要的代码，下一步是

我们如何让程序运行得更快

特别是，我们如何针对特定的机器对程序进行优化

也就是从对大多数机器的优化，转移到了对

特定机器的优化

针对特定机器优化是一个冒险的事情，因为例如 x86 的机器

在每个时间点，它们都有各种不同的型号

并且不同的型号也随着时间的推移而发展

因此，可能在一个 x86 处理器的特定型号上运行速度较快

但在另一台 x86 机器上运行速度可能较慢......

当你把优化的程序移到另一个机器时，你会发现你的努力有点浪费

另一方面，我将要描述的这些一般性想法实际上适用于各种各样的机器

随着我们的进展，我会更多地谈论这个问题

以前

如果你希望程序快速运行，则你必须编写汇编代码

而这显然不再是真的，如果有人告诉你这是真的

这是因为他们对这个不了解，事实并非如此

除非程序运行在资源非常小的机器上

比如程序运行在一个非常小的计算力不足的嵌入式系统上

我们使用的编译器为 gcc

我们这个课程使用 gcc 作为编译器，是因为 gcc 的获取比较方面

它实际上并不是最好的编译器，但英特尔制造的编译器需要花钱才能使用

虽然有其他编译器，但事实上 gcc 可以做到较好的优化

对于大多数人来说，gcc 是一个足够好的编译器

但是有些东西和特性编译器是不能真正理解的

例如，编译器并不真正理解你正在使用的数字

当你定义一个 int，但实际上它的取值范围是比 int 范围小很多的子集

编译器也很难理解内存引用模式和过程调用的影响

一般来说，编译器

有一整套优化策略

以及如何使用这些优化方案的方法

但总的来说，如果编译器如果对代码

能够进行某些优化不够确定

编译器不会优化，会采取直接的实现方法

讲的过程中，我会展示这方面的例子

编译器的优化方案中总有一个备选方案，即不优化

如果你希望程序运行得更快，你有可能遇到麻烦

因为编译器选择了不对代码进行优化

这时你可以使用一个非常有用的技巧

因为你可以阅读汇编代码，所以你可以编译程序

看看编译器做了哪些优化

如果编译器没有做到你想要的优化，你就回去看代码，找到原因

因此，用同一种语言重写程序

并对其进行调整以使其运行得更快，更加编译器友好

这是一种常用的优化程序的方法

只要你不把这个程序改的完全无法阅读

所以我们只讲一些通用的优化方式

这里的汇编代码的一些版本我们前面已经看过了

我们使用的大多数例子是多维数组

因为多维数组是相当容易优化的任务

但这些优化方法也适用于其他的程序

我们已经学习过了，怎么对一个多维数组中的元素进行索引

C 原来的代码风格是，如果你有一个二维数组

程序员可以将第 i 行第 j 列转换为一维数组中的位置

这个转化的公式是，每一行的列数乘以 i 加上列号 j

所以这是非常常用的代码

所以会抽象出这样一个函数

将二维数组 a 中的一行设置为一维数组 b 的值

这是你要编写的函数

我们主要观察这个循环体

唯一变化的变量是 j

所以从数组的角度来看，n*i 这个计算...

如果它在这个循环中反复执行

仅仅只是一种浪费

所以你可以一种叫做代码移动的优化方式

在循环外预先计算 n*i 的值

然后在内部一遍又一遍地使用它，编译器通常会这样做

当编译器可以检测到它是一个数组访问代码，并且编译器有这种技术时

如果你将 gcc 的优化级别设置为 1 或更高，它通常会进行这样的优化

我们实际上可以查看使用 gcc，优化级别为 1 编译后的汇编代码

并且你看到在访问数组元素之前，即循环体之外增加了这个乘法操作

实际上这个代码还做了另外的优化

它将代码转换为指针风格的代码

使用指针来访问数组 a，然后每次循环，指针的值加 1

另外，我们可以看到

gcc 通过移位和加法运算来实现

一个操作数为常数乘法和除法

我们已经看到过这样的例子，类似的

在之前的程序中，这个方法会应用于每一次对数组的访问

现在，我们要将二维数组 a 的每一行设置为一维数组 b 的值

然后，如果我们可是使用左边的代码，我们预先计算了 n*i 的值

所以现在内循环效率不错，但你马上意识到第一个乘法也是不必要的

因为从 i=0 到 i=1 到 i=2

因为我们可以通过让 ni 增加 n 来实现

这个称为计算量的减少，我们将乘法转化为加法

因为有一些可预测的模式可以更新这个变量 ni

这个例子也是关于数组访问优化的

想象一下，我们有一个图片，我们表示为像素值的二维数组

我们想要做一些过滤操作

我们想要得到一个像素东南西北四个邻居的像素值

求它们的平均值或它们的和

实现这个功能最自然的代码

是直接求出

上下左右四个像素值

类似于左边的代码，然后直接编译它

会发现汇编代码中有三个乘以 n 的乘法

i-1，i+1 和 i

如果编译器不是太聪明，它不会意识到它们彼此相关

对每一个像素，都会做 3 次乘法运算

如果我更聪明一点

我会手工重写这部分代码

然后重新编译

改动之后的代码如右图所示，inj 是 i*n+j

我可以通过加减 n 来得到下上像素的值

然后重新编译代码，这次编译的代码只有一个乘法运算

一般来说，乘法操作非常昂贵

但现在有足够的硬件资源，乘法操作需要大约三个时钟周期，所以这不是一个大问题

但是，任何时候你可以使用一个乘法操作替代三个乘法操作，都是有优化的

请讲

[学生提问]

这个问题是，如果你是针对空间进行优化，该怎么做

的确，程序有很多优化方式

为了更快地运行，可能需要编写更多代码

这里虽然左边的代码更短

但这里关心的是汇编指令的数量

在过去，程序占用内存的大小可以是一个比较关注的点

因为原来的 IBM 电脑只有 640k 字节的内存

并且是最高的配置，而且这个电脑的价格并不便宜

所以，过去程序占用的内存大小的限制比较大，但现在

程序的内存大小通常只是你电脑内存的一小部分

但这是一个有意义的问题

好的，展示这个例子，一般的编译器都非常擅长

这种低级优化，如果你编写代码的方式比较合理的话

但是还有一些其他的代码，你可以购买的最好的编译器也可能无法优化

我喜欢用这个例子来说明这一点......213 课程的第一个学期

我在实验室里看到一些学生写的这段代码

我对这段代码感到震惊

我向助教展示了这段代码，但没有一个人弄清楚出了什么问题

我已经向许多其他训练有素的 C 程序员专业人员展示过它

他们感觉没什么问题

那么让我们弄清楚为什么我被这段代码吓坏了

这段代码的功能非常简单的，有一个字符串 s

我想将该字符串中的所有字符转换为小写字母

所以我只是遍历这个字符串，对于字符串的每个位置

测试那个位置的字符，如果它在大写字母 a 和大写字母z之间

然后我将它变换到 a 和 z 之间，也就是转化为对应的小写字母

非常简单明了

但如果你运行它，你会看到

如果你的字符串到了五十万个字符

需要 240 秒左右，也就是 4 分钟，才能运行完此代码

这是一个比较大的字符串

但它真的不是一个特别大的字符串，你应该能够在 4 秒钟之内

将这个字符串转化为小写

你也注意到这种增长是非线性的，它是二次的

它正在以字符串长度的平方增长，所以这并不好

这是一类非常容易有隐藏的性能上的 bug 的程序

bug 会使程序的增长是二次的

并且当你的测试数据是 10,000 或更少的字符串时

它看起来并不是什么大不了的事，因为运行时间是微不足道的

但如果这个程序遇到了字符串比较大的情况

就会出问题

所以，为什么这个程序不好

关键的点是在条件测试中调用 strlen 函数

但 strlen 函数是通过测试字符串是否到达末尾的方式来计算字符串的长度

现在，我们将 for 循环转换为 goto 形式

有各种方法来进行这个转化

但是所有这些测试都在循环中

因此，每次循环时都会产生对 strlen 的调用

程序员常常忽略了这样一个事实

for 循环有三个不同部分

初始化只执行一次

但测试和更新每次循环都会执行

因此，strlen 调用的次数，等于字符串中字符的数量

现在看 strlen 如何工作

查看字符串长度的唯一方法

是遍历整个字符串直到找到空字符为止

因此 strlen 本身就是线性时间的操作

所以，你对一个线性时间函数调用了 n 次 

执行的过程中字符串越来越短，但是执行的速度不是很快

基本上是二次的

这就解释了为什么我们的运行时间会比较长

如果我做了以下一些小改动

我引入一个名为 len 的局部变量

我预先计算 strlen

因为字符串的长度没有改变，我们只是改变了这个字符串中的字符

那么程序将做同样的事情

但是现在运行时很短，甚至都看不出来时间的增长

即使是一百万个字符，也不是什么大不了的事

因为它仅仅是遍历字符串

这只是我职业生涯中见过的许多例子之一

看似无足轻重的事情，最后证明存在严重的性能问题

那么为什么编译器无法解决这个问题

为什么编译器不能更智能一点

看看原始代码

知道这是程序员写的

但编译器知道更好的方法，会提前预先计算 strlen

有几个原因使得编译器无法进行此类优化

第一，这里的代码在循环中修改了字符串

这里我的意思是这里的代码是修改字符串，并且我们调用了 strlen

因此，编译器必须非常小心的分析，知道

即使字符串正在改变

从 strlen 获得的结果不会改变

所以这是一个原因

第二是......

那么编译器如何确定哪个版本的 strlen 实际上将被使用

因为每个文件都是单独编译的

编译之后，它们才会在链接阶段链接在一起

其中一些甚至在程序启动后发生

所以尽管有一个标准的 strlen 函数

实际上并不一定会在最终的程序中使用它

所以编译器真的不能确定调用那个 strlen 函数

想想一下，我提供了一种这样的自定义 strlen 函数（如 PPT 所示）

size_t lencnt = 0;
size_t strlen(const char *s)
{
    size_t length = 0;
    while (*s != '\0') {
	s++; length++;
    }
    lencnt += length;
    return length;
}

它记录调用它的的所有字符串的长度之和

或者其他一些副作用

那么程序会产生一个非常不同的结果

如果我进行优化的话

所以编译器必须假设 strlen 只是一个黑盒子，它可能做任何事

并且不能对它能否产生的副作用做出任何假设

所以即使使用最好的编译器，它也不会在任何机器上进行这样的优化

所以这只是一个例子，你可以告诉我，你已经对此敏感了

所以你发现了这些，但很多人没有

嗯，让我们看看这个

嗯

哦，这是一个糟糕的代码的另一个例子

我有一个二维数组 a 和一个一维数组 b

我想使 b 的 b[i] 等于 a 中 i 行中所有元素的总和

这里这个程序写得相当直白

b[i] 初始化为 0，然后遍历 a 的一行，把 a[i][j] 的值累加到 b[i] 上

当然，我们现在知道我们可以通过移动 i*n 来改善这一点

但我想说的不是这一点

主要到在程序的内循环中

我们已经简要介绍了一些浮点指令

它们的主要特征是

移动指令看起来像你熟悉的移动指令

除了我们将浮点数据放在其中一个 ％xmm 寄存器中

所以你在这里看到的主要是它从内存中读取数据

然后加上某个值，然后它写回内存

那个内存位置对应 b[i]

所以这意味着每次循环都需要从内存中读取 b[i]，然后再把 b[i] 写回内存

除了多了内存读取之外

b[i] 与上次循环

更新的值相同

那么你为什么要这样做，为什么要从内存中读取然后再写回到内存呢

然后再从内存中读取，加上某个值，再写到内存中

为什么必须一遍又一遍地在内存和寄存器之间来回传送数据

原因是因为在 C 中你无法确定是否有

内存别名使用

想象一下，如果行 b 为 a 中的某一行

好吧，想象一下，在 C 中你可以这样做，这是合法的 C 代码

你可以使一个内存数据结构覆盖另一个数据结构

当程序的不同部分指向内存中的相同位置时，这称为别名

并且 C 编译器无法知道是否存在别名

编译器需要做大量的工作来检查是否有内存别名引用

但总的来说，编译器假设存在内存别名引用

想象一下，数组 b 可以对应于数组 a 中的一行

所以 b 的初始值是 4，8，16

但是如果你逐步调试这段代码

你会发现它的行为比较奇怪，可能对任何事情都没用

它仅仅表明，当 b 更新的时候

它改变了 a，然后影响了求和的结果

在 C 语言中这是可能发生的

所以编译器对于这样的代码

它必须假设这两个存储位置可能相互重叠

所以，编译器会小心地把值写到内存然后一遍又一遍地读回来

因此，如果通过引入局部变量来重写此代码

并把求和的结果保存在该局部变量中

然后只在最后我赋值 b[i]

然后你会看到这个完全相同的循环突然变得更加简单

它只是读取一个浮点数，然后加到寄存器

这里，对内存的读写限制了程序的性能

所以改写之后不会明显加快

再一次重申，作为一名程序员，你很难想到这是一件大事

但是 C 编译器通常不能进行这样的优化

因为它无法预先确定是否存在的内存别名使用

因此，要习惯这两个例子中引入局部变量的写法

这样可以告诉编译器不要一遍又一遍地调用相同函数

不要一遍又一遍地读取和写入相同的内存位置，只需将其保存在临时位置即可

然后编译器会自动分配一个寄存器并将结果存储在该寄存器中，一切都会很好

好的，这就是会阻止编译器优化的一些东西

作为程序员，你可以帮助编译器进行优化

影响编译器优化的，主要是内存别名使用和

函数调用中的副作用

好的，现在我们要转移到另一个话题

这位同学有问题吗？[学生提问]

不，这是 C 代码（double B[3] = A+3有问题，GCC 编译无法通过）

我确定这是 C 代码，因为我运行过它

这个函数接受两个 double * 的参数，所以它们都不是二维数组

所以这就是说 a 不是二维数组

可以把数组 b 想象成 9 个元素数组中的其中 3 个元素

加三表明是从第三个开始

所以这就是声明 b 不是指针它是一个数组

但数组的名称也可以作为一个指针

可以通过指针读写数组

[学生提问]

对，我会仔细检查仔细检查这段代码

[学生提问]

所以问题是为什么 C 使用 null 来标识字符串的结尾

使用这种方式可能是一个糟糕的决定，但多种原因

我想，因为 C 语言是由 D.M.Ritchie（补充的）发明

他曾写过很多汇编代码，想要提升汇编代码的抽象级别

因为他们不想一遍又一遍地写同样的东西

所以他们进一步考虑高级的抽象

他们只是试图在机器级编程之上提供一个小的抽象

可以使他们编写的代码，从一个机器跨越到另一台机器

所以，他们设计的 C 语言，都使用最简单的表示方式

例如，大多数语言，数组都有边界检查

数组是一个数据结构，包括它的大小，值的范围和其他的东西，而 C 没有这些东西

C 语言值提供基本的功能

而且你知道 C 语言已经存在了 40 年左右的时间

[学生提问]

C 语言参考了 Pascal 语言，这不是真的

Pascal 语言是 Niklaus Wirth 发明的，用于教学用的语言

它是用于教学的一种语言

所以它是帮助学生理解编程

C 由专业程序员设计，帮助他们编写代码，不是为了帮助学生学习

所以这两种语言之间的理论区别比较大

学生提问

对不起，哦，是的，那是一个错误

我会仔细检查这个，你们可以检查一下，我很确定这个代码是没问题的

如果有问题，可以改为 double *B = A + 3，这样改了之后没问题了

你有什么问题吗

[学生提问]

你觉得......

 -你认为这里应该 28 -是的 -我不会尝试在这里执行这个程序

但我们会仔细检查这个代码

如果有错误的话我们会修复它，谢谢你指出这个问题

是啊

[学生提问]

不，这在代码中完全没有任何区别

在栈上分配内存还是在堆上分配内存在这里完全没有区别

我会检查一下，这里可能是 double *B=A+3（PPT上应该是写错了，编译通过不了）

这是我不久前写的代码，但我会仔细检查它

好吧，无论如何，这种简单的优化往往是在最后才做的

它们是简单的优化，但你必须养成这样做的习惯

好，我们现在来看另一种比这个有趣的优化

正如我所说，这种优化更依赖于系统

但是现在几乎所有处理器都实现了这个特性

名为的无序执行的特性

除了最原始的微处理器

所以这是一种可以

适用于各种各样的机器的通用优化方法

我要通过一系列例子来说明这种优化方法

从一些不是非常高效的代码开始，使其运行得更快更快

优化之后的速度将达到 40 左右

仅仅只是做这种优化

首先我定义一个数据结构

这看起来像 pascal 语言实现数组的方式

我没有说 Pascal 语言不好

过去我们曾经教过这种语言

因此，在一种语言中实现数组的典型方法是

使用一个数据结构不仅保存数组中的值

还保存与之相关的其他信息，例如它的大小

所以这是一种很好的抽象方式

你编写的代码可以确保，如果你对数组的访问越界了

会返回一个错误信号

/* data structure for vectors */
typedef struct{
	size_t len;
	data_t *data;
} vec;


int get_vec_element
(*vec v, size_t idx, data_t *val)
{
	if (idx >= v->len)
		return 0;
	*val = v->data[idx];
	return 1;
}

你看到的这个函数的功能是，从数组中取出索引值对应的元素，传递一个指针

然后该指针被赋值为数组中索引对应的元素

此函数的返回值为 0 或 1 0 表示失败，1 表示成功

我将元素的数据类型定义为 data_t

这样我可以修改 data_t 的定义，然后重新编译

data_t 可以定义为 int，long，float 和 double

我们将看到性能特征如何随着不同的数据类型而变化

void combine1(vec_ptr v, data_t *dest)
{
    long int i;
    *dest = IDENT;
    for (i = 0; i < vec_length(v); i++)
    {
        data_t val;
        get_vec_element(v, i, &val);
        *dest = *dest OP val;
    }
}

我使用的基准测试非常简单

对于这个数组

我想计算数组中所有元素的总和或累积

同样，我在这里使用了两个宏定义 IDENT 和 OP

OP 定义为加法，且 IDENT 定义为 0

或者 OP 定义为乘法，且 IDENT 定义为 1

这样我就可以对两种运算进行对比

所以，我们有八种可能

四种不同数据类型和两种不同的运算的组合

基准测试是以最直接的方式编写的

我先使用 get_vac_element 的函数来得到数组的第 i 个值

然后累加或累积到 dest

现在来看一下这个的性能

我们将引入的一个叫 CPE 的指标

它代表处理一个元素所花的时间周期

之所以用这个指标，是因为通常当你编写代码遍历处理一个数组时

随着数组的增大，处理的时间是线性增长的

你并不关心对一个元素的处理需要多少秒或多少微秒或多少纳秒

你想知道它的整体性能特征是什么

进行低级代码优化也是如此

使用处理器内部时钟的时钟周期作为时间单位更有用

而不是像纳秒这样的时间单位

因为处理器是以 2G 赫兹运行还是 2.3G 赫兹运行

一名程序员是无法控制它的

但程序员可以控制程序中不同的计算部分使用了多少个时钟周期

所以这就是为什么它被称为每元素的周期数

这里展示了函数实际测量的 CPE 值

但通常像 combine 这样的函数会有一些额外的开销（类似 size 为 0 时的开销）

一个固定的开销，因为需要设置循环，调用函数和其他的一些东西

然后是一个线性分量

我想知道的是线性分量的斜率

斜率就是每个元素的周期

你可以认为是向数组添加一个元素的增量

所以现在如果我运行这个函数

我这里只展示四种结果

因为事实上，data_t 是 int 还是 long 还是 float 或 double

大多数情况下都不会对性能产生影响

好，如果我指定优化等级为 0

每个元素需要大约 20 个时钟周期

如果我优化级别设置为 1，它只需要花费不优化时一半的时间

因此，只需更改编译器的优化选项，我就可以将每个元素降低到 10 个时钟周期

这个例子所用的代码是我能想到的，最不优化的代码

然后我们可以进行一些优化...

我们可以像前面描述的那样，减少这个程序中的一些冗余

把程序变的更简单一点

前面看到，每次调用 get_vec_element 函数

该函数都会检查数组索引是否越界

一遍又一遍地检查同一个数组是否越界是很愚蠢的

所以当我遍历时，我先得到它的长度来确定要访问的元素数量

放弃 get_vec_element 函数的边界检查

我可以引入一个函数，它返回数组的长度

数组长度之外的东西就直接忽略

所以我可以重新编写一个循环，引入局部变量

先把数据累加到临时变量，然后再赋值给 dest

然后程序实际上变得更快了

重申一下，这里使用的是优化级别 1

它把整数的加法降低到 1 个多时间周期

整数乘法降低到了 3 个时钟周期，双精度乘法降低到了 5 个时钟周期

所以这很好，肯定有所改进

但问题是，最好的

先解释一下这三个数字怎么来的，3，5

和 1.27

这些数字的来源暗示我的程序存在一些基本限制

要理解这些数字的来源，你必须对底层硬件有一些了解

你可以选修一个非常好的课程，ECE 741

它会教你，你能想象的，有关处理器设计的一切知识

你会自己动手设计了这样的处理器

但我想你暂时不打算那样做

因为这门课程有 7 门先修课程

那么，让我给你一个简单版本的处理器

这个处理器有点像 1995 年左右的处理器

所以这是旧的东西，但真正理解它

非常困难，有很多细节

因此，即使是 ECE 447 这样的计算机架构课程也没有讲述这方面的内容

他们并没有真正进入这种设计

因为它们很难，要自己设计非常困难

但基本的想法是，一个程序

计算机会读入一条指令，然后执行这条指令

读取另一条指令，然后执行那条指令

这实际上和正在执行的程序无关（程序可以分解为基础指令）

CPU 提供了这个庞大的硬件基础设施

使程序运行速度比

一次只执行一条指令快

它采用了一种称为超标量乱序执行的技术

这个想法粗略地讲

你可以认为你的程序是一个顺序执行的指令序列

CPU 尽可能的读取多的指令序列

然后 CPU 把读入的指令拆开，发现有的指令之间

不是相互依赖的，所以我可以开始执行程序后面的代码

而不是当前的代码，因为它们彼此独立

这被称为指令级并行性，也就是

即使你的程序是一个顺序的指令序列

但实际上，这些代码可以拆分成不同的部分

某些部分相互依赖，某些部分不相依

CPU 有一堆硬件

PP T上图的上半部分显示了获取指令的方法

这里有高性能高速本地缓存

这个缓存只是尽可能快地提取你的指示

然后缓存把指令送入指令译码这个硬件中

这个硬件把指令拆分为低级操作，并确定这些操作之间的依赖关系

CPU 还有一组功能单元

能够执行这些低级操作，如算术浮点运算

从内存中读取数据，写入数据到内存

所有的这些操作都使用缓存，你们很快就会学缓存

缓存可以视为某些内存数据的高速副本

总的来做，基本的思想是把你程序的操作进行拆分，重组

使这些基本单元尽可能保持繁忙

执行代码的不同片段，与以前不同的顺序执行不同的指令

一组寄存器可以看做是可读可写的一段内存

一个寄存器相当于一个内存符号（寄存器重命名，P360）

一条指令往写个内存写，其他一些指令读取这个内存

寄存器是一条指令产生数据的目的地，也可以是一些其他指令的数据来源

有很多数据通过寄存器进行传递

可能是一次计算结果，作为另一次计算的输入

基于寄存器名称而不是将它们显式存储在寄存器文件中

有一个寄存器文件，当计算的结果出来了，它们将被存储在这里

无论如何，这里有很多东西

但主要考虑的事情是，你的机器有足够的资源，可以同时进行多项操作

但这需要你能以某种方式构建你的程序，才能同时使用这些资源

可以同时进行多项操作的 CPU 被称为超标量指令处理器

这种 CPU 可以在一个时钟周期执行多条指令

实际上，1993 年，Intel 的 Pentium 处理器就可以同时执行两条指令

紧接着他们推出了一款名为 Pentium Pro 的产品

它是所有现代处理器的基础

顺便说一下，Intel 的首席架构师是 CMU 毕业生

乱序执行是现在处理器的模型

乱序执行比较复杂，但功能单元可能比你想的更加复杂

功能单元使用了流水线技术

流水线的基本思想是将计算分解为一系列不同的阶段

一个简单的例子是你想要计算 a*b+c 的值

你先做乘法，然后做加法

但乘法器做乘法比人算乘法更加复杂

乘法器先将乘法分解为可以一个接一个地完成的

更小的步骤

你可以认为每个阶段都有一个单独的专用硬件

然后你可以做流水线操作

也就是，当一个操作从一个阶段移动到下一个阶段时

前一个阶段空出来了，你可以填入新的数据

请看 PPT 上的例子，我的乘法器分为 3 个阶段

我计算 a*b，a*c 然后把两个积相乘

要注意的是 a*b 和 a*c 不以任何方式相互依赖

所以我可以同时计算它们两个的积，但我没有两个乘法器

但我可以一个接一个地做一个

所以，Time 1（PPT 上的表）我可以先计算 a*b 的第一阶段

当 Time 2 时，它将继续进入第二阶段

到了 Time 3，它将进入第三阶段

但 Time 2 时，我可以同时计算 a*c 的第一阶段

因为这个阶段是空闲的

当 a*b 从第 1 阶段移至第 2 阶段时

所以在其他操作后面的一个时钟周期，我都可以开始新的操作

现在 p1*p2 显然取决于 a*b 和 a*c 的乘积

因此，在 a*c 完成之前 p1*p2 无法启动

然后我们计算 p1*p2，后面没有其他的操作，所以流水线后面是空的

总的来说，我们已经完成了看起来需要 9 个步骤的乘法（3 个乘法 *3 个时钟周期）

但由于流水线操作，这里总共有七个步骤

这位同学有问题吗

什么...PPT 的这个表

如果在这些不同的地方有不同的乘数，你可以同时执行

每个阶段是完全相互独立

[学生提问]

是的，这一切都在单个处理器的单核中，

单核的这种并行性是一种比多核低的并行性

除了在低功率的嵌入式处理器中，其他的处理器都提供了这种并行性

并且大多数时候你的硬件没有得到充分利用

这就是流水线操作的基本思想，它有点像并行性

但这种并行性不是说你拥有多份资源（如多个乘法器）

而是把在单个硬件上的操作，划分为紧密联系的顺序的多个步骤

嗯，下面将 Haswell CPU，它比 shark 机器（他们上机实验用的机器）要新一点

但也查不了太多

它是英特尔 x86 系列的最新版本之一

Haswell CPU 有许多不同功能的功能单元

这些功能单元组合起来，可以同时执行 2 次读内存，1 次写内存

4 次整数运算，两次浮点乘法，一次浮点加法和一次浮点除法

这些操作不可能同时发生，因为功能单元之间有一些共享资源

但功能单元的确比较多

而你也可以测量....

指令有两个参数，延迟（Latency）是一个指令从头到尾需要多长时间

但由于有流水线操作，还有一个参数（Cycles/Issue）表示两个小步骤之间的距离

从 PPT 上我们可以看到，大多数操作需要花费几个时钟周期才能完成

但它们采用了流水线的技术，流水线的一个步骤

只需一个时钟周期

注意到，除法操作非常慢，并且没有流水线

所以，相对而言，在大多数机器上，除法是一项非常昂贵的操作

我讲这两个参数的原因是，它们限制我们程序的运行速度

举个例子，如果我计算多个整数乘法

.L519:		# Loop:
	imull	(%rax,%rdx,4), %ecx	        # t = t * d[i]
	addq	$1, %rdx	                # i++
	cmpq	%rdx, %rbp	                # Compare length:i
	jg	    .L519	                    # If >, goto Loop


PPT 的上方是它的代码

从代码中可以看出，在我开始下一个乘法之前，我需要上一次乘法的结果

每次乘法需要 3 个时钟周期，所以这里得到的结果是 3 个时钟周期

实际上我的测量结果都符合

我称为机器延迟的限制

这种限制是基于一个操作从开始到结束需要的时间

如果我们把程序的计算顺序画成 PPT 左侧的这张图

程序进行一系列的乘法运算

并且在我开始下一个之前，我需要上一次乘法的结果

如果你看一下这个循环代码

它必须先计算乘法，更新 %ecx 的值

然后更新 i 的值，再开始下一次循环

所以这就是为什么，即使我有一个流水线乘法器

但我的程序本身限制了我的所有乘法必须顺序执行

所以让我们看看我们否能超越那个延迟界限

这里采用了一种技术，你可能以前听说过，叫做循环展开

循环展开的基本思想是

在循环中计算多个值，而不是一个值

因此，PPT 上的代码展示了 2*1 循环展开

也就是每次循环我处理数组的两个元素

在循环内部，我计算 x 和 d[i] 及 d[i+1] 的和或乘积

我必须加入一些额外的代码才能完成

如果循环展开值比较大呢

我想你明白了

这里我展示了 2*1 的循环展开

但你可以想象，这适用于不同的循环展开值

这对我们有什么帮助

当我运行它时，整数加法的速度提升了

但其他的根本没有改善

所以这个更快

但这个是因为原来的代码在循环的计数上的开销比较大

因为原来的代码已经接近一个时钟周期

所以我是要超越这个特定指令的延迟界限

但它没有...主要原因是我这里的代码也需要顺序执行

为了获得 x 的新值，我必须先

乘以 d[i]，然后乘以 d[i+1]，然后开始另一次循环

但我可以做一个非常小的改变，就可以让性能显著提升

我把这些括号移到右边（代码中红色的部分）

改了之后有什么不同呢

你可以发现运行的时间减少了

我把这种展开称为 2*1 循环展开

我会在一分钟内解释它的含义

我这里用小写字母来表示我做的这个基于结合律的转换

你可以发现这样转换之后，三种情况下时间都变为了原来的一半

所以这种转换一定改变了一些东西

现在我来说明为什么会这样

你们想想没有修改的代码

并考虑一下计算的顺序

你会发现我改变了计算的结构

改变之后我先计算数组中的两个元素的乘积

然后将它们累积到整体的积中

所以，实际上括号的转移改变了

计算的方式

你现在可以看到，在这个例子中，决定性能

的关键路径已经变成了左边这张图

它的关键路径恰好是前面的一半

这就是我现在的运行速度是前面的两倍的原因

但这个转换没有提升整数加法的性能

但是对于其他三个运算，速度已经提升了一倍，仅仅是改变了括号的位置

现在这里有一些好消息和坏消息

好消息是，如果这是整数运算

你们已经知道了，二进制补码的加法和乘法运算满足交换律和结合律

因此，整数的乘法和加法进行这种转换都是没问题的

所以我可以将这些元素以任意的顺序组合

无论怎样组合，我都会得到完全相同的答案

但是你知道对于浮点数的运算，并不满足结合律

所以对于浮点数运算，如果移动这些括号

由于可能出现舍入（如大数加小数，小数会被忽略），甚至是溢出的情况

可能会导致计算的结果不同

但如果在计算过程中不会发生舍入的情况

那么这种转换不会影响你的计算结果

但即使发生舍入的情况比较少，但也足够让大多数的编译器

不会改变浮点数的结合性

因为它们对浮点数计算的优化非常保守

应用程序员，你必须要充分了解这些东西

知道做某个转换是不是可能导致错误的结果

现在有一套新的界限

这个界限是你程序能够达到了最好性能

延迟界限是指在一系列操作必须严格顺序执行时，执行一条指令所要花费的全部时间

但还有一个更基本的界限，我称之为吞吐量限制

这个限制是基于硬件的数量和性能

基于功能单元的原始计算能力

例如，这两个操作的吞吐量界限是 1（上一张 PPT）

因为它受限于下面的要求

我必须把数据从内存中读出来

我有两个不同的计算单元

没有！对不起，我只有一个整数乘法器，一个整数加法器

这两者的吞吐量实际上只有 0.5

因为这是硬件设计的一个奇怪的地方，它有两个两个浮点乘法器

但只有一个浮点加法器

所以，这里乘法实际上比加法运行得更快

这里虽然我有 4 个加法器，但只有两个加载(load)单元，所以性能受限于加载单元

因为我必须先从内存中读取一个元素

所以我不能低于 0.5

我们现在来看看这段代码实现的转化

它使我们突破了延迟界限，接近于吞吐量界限

这是另一种可以获得更多并行性的技术

我称之为多个累加器

想象一下

我们在数组中有索引为奇数的元素和索引为偶数的元素

我们可以分别计算这两组元素的和或积

然后最后将它们组合在一起

所以这是另一种形式的结合律变换

这改变了元素组合的顺序

我们将索引为奇数的组合在一起，索引为偶数的组合在一起

一般来说，我们可以通过参数 i 来将数组划分为多份

它和前面的转化有同样的问题，如果它是整数运算，那很好

如果它是浮点数，则存在改变程序行为的风险

你会看到整数除加法外，其他的操作运行时间是原来的一半

但比整数的加法要慢一点

我们看看 PPT 左边的图

它表示了我们的计算顺序

我们在这里计算所有偶数

把偶数编号的元素组合在一起，这里所有奇数编号的元素

最后，我们将二者的结果组合在一起

进一步的，我们可以将其一般化，我们可以以 K 为因子展开一个长度为 L 的数组

这样，我们可以并行计算 K 个值

我们可以使用不同的 l 和 K 值

一般来说 L 必须是 K 的倍数

你可以更改 K 和 L 的值，然后运行程序，这里是浮点数乘法的结果

可以看到，实际上运行速度可以接近吞吐量极限 0.5

这是整数加法的结果

运行速度也可以接近 0.5

一般来说，通过选择最佳参数

我们的运行速度可以非常接近这个处理器的吞吐量界限

还记得吗，原来这个程序的 CPE 是 20 个时钟周期和 10 个时钟周期

现在我将 CPE 降低到了 1 和 0.5

但我们还可以把它降得更低，现在来看最后一步

讲浮点代码时，我提到过有一组特殊的寄存器

在 x86 的机器上，我们称为 ％xmm 寄存器

而现在新一代的 CPU 上，增加了一个叫做 ％ymm 的寄存器

它的大小是 ％xmm 寄存器的两倍

所以这些寄存器有 32 个字节

并且 Intel 会在一年内推出一个新的名为 AVX512 版本

寄存器为 512 位，因此长度为 256 字节

512 比特，对不起......长度应该是 64 字节

64 字节，所以它的长度是 %ymm 长度的两倍

正如我之前提到的，你可以将它们视为 32 个 1 字节的整数

或者把它们视为浮点数

好，现在

浮点数使用这些寄存器的低 4 位或低 8 位

但是，我们有一些称为矢量加法的指令

其中一条矢量加法指令具有执行八次单精度浮点加法的效果

或者相当于 4 次双精度浮点加法

但这个硬件很少处于繁忙的状态

如果这个硬件繁忙起来

这样可以较大的提升乘法性能

三个时钟周期，流水线的浮点数乘法

你可以在三个时钟周期内，并行进行八次浮点流水线乘法

正如我所提到的，shark 机器是一个早期版本，它的 %ymm 寄存器的长度是这里的一半

因此它可以同时执行 4 个单精度或两个双精度的浮点数运算

如果我把代码改写为矢量代码

然后你可以看到 CPE 降低了 4 倍

所以运行速度更快了

这个 0.06 是 0.0625

它每个时钟周期执行 16 次操作（原来每个时钟周期执行 2 次操作，提升了 8 倍）

但是我们最多只能接近矢量吞吐量界限

但总的来说，运行速度已经提升了很多

所以人们真的很关心

你可以想象，这些指令是为视频、声音、图象处理而引入的

这些信号处理的程序对性能的要求很高

如，显示图像的速度有多快，将某个图象旋转速度有多快

渲染图象速度的快慢影响很大

在电脑游戏中这是一个关键因素，在其他领域也比较重要，如你拍摄一张照片

这些指令就是为了这些情景设计的

人们为这些应用程序编写代码

他们需要编写这种向量化的代码

不幸的是，虽然 intel 编译器会自动做一些这方面的优化

但 gcc 虽然尝试去实现这种优化

但实际效果不好，可能 gcc 已经没有继续这个优化了

这里有一个网络旁注

对应的网页上讲了

如何进行这种向量化的编程，有兴趣你可以试试

网页上还有一些 gcc 的扩展

但是你可以编写代码

然后利用 gcc 的扩展（补充）编译，这样就可以利用这些矢量指令

这是我的做法，也是我得到这个性能的测试结果的方法

这是我要展示的结果

还有一点，这种优化是针对某个机器的

如果你要在不同的机器上运行程序，你需要调整 gcc 的选项

这种优化依赖于 gcc

如果你想自己编写代码，你可以试一试

现在我们来讨论另外一件事情

前面为了简化，我们程序是一个非常长的顺序结构的指令

然后我们的 CPU 一次读取尽可能多的指令，然后把这些指令分开

但是，我们这里的程序实际上是一个循环

并且循环中的指令不多

那怎么把它变成一个顺序指令呢

这依赖于处理条件指令的方法

所以，现在你知道，CPU 会读取指令

如果指令中有跳转指令，CPU 会跳转到对应的分支中去执行

这里就出现了一个两难的境地，因为这一个分支也可能被执行

抱歉，这里的意思是，如果条件满足，它会跳转到对应的对应的分支

如果条件不满足，它会继续执行后面的代码

并且我们没有办法事先知道那个哪个会被执行，因为这与数据有关

所以，在现代处理器是这样处理这种情况的

它采用所谓的分支预测，但本质上只是猜测

猜测哪一个分支会被执行

然后 CPU 会执行对应分支的代码

但执行对应代码的时候，必须能够保证

没有对该程序造成不可修复的损害

我们会看到这意味着什么

指令控制单元（上半部分）中有许多逻辑单元

指令高速缓存会读取指令

如果遇到了分支，会先执行某个分支，当执行到后面（乱序）

发现预测是正确的，就会接着往下面指令

如果预测不正确

你可能要回到 100 个时钟周期前

这不是很久以前的事

你需要回到分支点，然后开始取出和指令另一个方向上的指令

因此，跳转指令的指令变成了这样

先执行猜测的分支，然后在后面判断猜测是否正确

所以总的来说，如果你它会以一种方式预测并开始执行一个分支

举个例子，这里有一个循环

预测不会跳转，执行后面的内容，然后会再次回到循环的开始

实际上，这是一个很好的猜测

因为这执行到了循环的结尾

但是我们会继续这么猜

因此程序继续执行循环体的内容

通过这种方式，创建了一个长的顺序指令序列

功能单元可以读取并执行

一般来说，一些指令是有效的，而一些指令是多余的

因为已经完成了所有的操作

然后继续执行的时候，一个标志位会提示当前的分支不是无效的

然后它会返回

并且取消所有已读取并执行的指令

注意，这里所有的指令都只修改寄存器

并且，它有所有寄存器的多个副本

回退的时候，这些寄存器

每一次计算的结果都依次保存在寄存器副本中，所以，猜测正确的在前，猜测错误的在后

因此，当需要取消它时，只需取消所有待处理的更新

并把正确的值返回（前面是存储在副本中，并没有真正更新寄存器）

这位同学有什么问题吗

在 CPU 中，有一个寄存器重命名块

它对每一个寄存器都有多个副本，计算的结果就保存在这些副本中

[学生提问]

对于每个寄存器，它通常有几百个虚拟寄存器（副本）

用于存储需要更新到实际寄存器的值

[学生提问]

嗯，对，它记录

更新的次数，知道这是第一次更新，这是第二次更新，这是第三次更新

它记录了你可以想象的一切，这不是你在一学期课程中能够学完的内容

它需要记录所有的内容，所以，确保它能够正常工作相当棘手

但从概念上讲，这是一个非常简单的想法

它只是基于猜测提前做了许多工作

然后只有在出错的情况下才会

回退到某一个点

好像它只执行到了那个点

然后它继续执行

后面正确的分支

它是如何回退的，这是非常有趣且棘手的内容

我们在课程的早期谈过

关于条件传送和条件分支代码的区别（书 P145）

条件传送可以在管道的结构内进行

如果是条件分支代码，并且是一个不可预测的数据

可能会执行大量无效的工作

但更糟糕的是，当它不得不回来重新启动时

需要一段时间来填充系统中的所有缓冲区

才能继续执行后面的代码

这节课差不多要结束了

总结一下，首先，不要做任何愚蠢的事情，愚蠢可能说的有点过了

然后，记住，作为一名程序员，你一直在做的某些事情可能是有问题的

它们并不明显

然后，我们讨论了如何通过调整代码来获得一些指令级并行性

它们是和具体的机器相关的

但现在几乎所有的机器，都属于一类机器

所以，改变计算的结合性，这种技术

几乎对所有机器都是有效的

无论是手机中的 ARM 处理器

或者是笔记本电脑或 shark 机器中的 x86 处理器

它们都具有相同的实现结构

因此，这些技术将适用于这些所有的机器

好的，今天我们的课程就到这
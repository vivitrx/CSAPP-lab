Good afternoon everyone welcome to 213 it's good to see you

大家下午好，欢迎来到 213 课程教室，很高兴我们又见面了

Just a reminder that your attack lab is due tonight at 11:59 p.m.

提醒你们一下，attack lab 将于今晚 11 点 59 分截止

You have one grace day for this lab

这个 lab 你将有一个宽限期

And cache lab we'll go out it right about the same time

我们将在同一时间发布 cache lab

Now it's going to be a little tight for cache labs it'll be due next Thursday

cache lab 的时间有点紧张，它将在下周四截止

So you might want to you might want to get started on that soon

所以希望你们能够抓紧时间开始

Last lecture we learned about the memory hierarchy and the idea of caching

上节课，我们学习了「内存层次结构」以及「缓存」的概念

Today we're going to look at a very important kind of cache

今天我们将介绍一个重要概念「缓存」

Which are called cache memories

它被称之为「高速缓存存储器」

And they're very important to you as a programmer

作为程序员，它们对你非常重要

Because they can have such a big impact on the performance of your program

因为它们会对程序的性能产生巨大的影响

So if you know about these the existence of these cache memories and you know how they work

如果你知道这些缓存的存在，而且你了解它们是如何工作的

As a programmer you'll be able to take advantage of that in your programs

作为程序员的你，就能够在程序中发挥它的优势

So last time we looked at the memory hierarchy is a collection of storage devices

上次，我们说到内存层次结构是存储设备的集合

With smaller,costlier and faster devices at the top and slower cheaper and much larger devices at the at the bottom

越处于顶部的存储设备容量越小，越昂贵，速度也越快，处于底部设备则相反

And then at each level in this hierarchy

这个层次结构的每个级别

The device at level k serves as a cache holds a subset of the blocks of

层次 k 的存储设备作为高速缓存，储存着下一层次中的一部分子集

That are contained in the device at the lower level at level k+1

即层次 k + 1 的存储设备的子集

Now recall the general idea of caching so we have a memory

回想一下缓存的一般概念，假设我们有一块内存

It's an array of bytes and we break it up arbitrarily into a collection of blocks

它实际上是由字节「byte」组成的数组，但我们把他拆分视作为 「块」 的集合

And these this memory is larger slower and cheaper

这些内存更大、更慢、更便宜

And so it's,and it's much larger than than a cache which is smaller faster and more expensive

它比一个更小，更快，更昂贵的缓存的容量要大得多

And which holds a subset of the blocks that are contained in the main memory

并且它包含主存储器中所包含的块的子集

And then blocks are copied back and forth between the cache in the memory in these block size transfer units

在这些块大小传输单元中，块在存储器中的高速缓存之中来回复制

So for example if our program requests a word that's in contained in block number 4

例如，如果我们的程序请求包含在块编号 4 中的字

It asks the cache to return the word that's contained in block 4

它要求缓存返回块 4 中包含的字

The cache looks and it's at the blocks that it's the subset of the blocks

缓存在它的块的子集上搜寻这个块

That it's stored discovers that block 4 is not there

它发现块 4 不存在

So it asks the main memory to send it block 4

所以它要求主存储器发送块 4

Which it does and when that block arrives at the cache

然后存储器照做了，然后当这个块到达缓存时

The cache stores it but potentially overwriting some existing block

缓存存储它，但也有可能需要覆盖一些现有块

Similarly if our program asks for a data word

同样的，如果我们的程序请求一个数据字

If that's contained within block 10

包含在第 10 块中的数据

The cache looks sees that it doesn't have that block

缓存搜寻之后发现它没有该块

So it requests that block for memory which copies it into the cache

因此它请求将内存将该块复制到缓存中

Which overwrites an existing block

这将会覆盖现有的块

Now subsequently if our program asks for a request

如果我们的程序请求

If our program references a word that's contained in block 10

如果我们的程序需要引用块 10 中包含的字

For example then the cache then we have a hit and the cache can return that block immediately

这时，我们说缓存「命中」，缓存就可以立即返回该块

Without going through the expensive operation of contacting memory and fetching that block from memory

无需经历费时长的操作，通知内存、并从内存中获取该块

Now there's a very important class of caches these so called cache memories

现在有一类非常重要的缓存，即所谓的「高速缓存储存器」

Which are contained in the CPU chip itself

它包含在 CPU 芯片之中

And are managed completely by hardware

并且完全由硬件管理

And they're implemented using fast SRAM memories

它们是使用快速 SRAM 存储器实现的

And the idea for this cache which right next to the register file

处于「寄存器组」附近的缓存的实质是

Is to hold frequently access blocks or blocks from main memory that are accessed frequently

是存储主存储器中经常访问的块

Okay so hopefully because of the principle of locality

幸运的是，因为「局部性原则」

Most of our requests for data will actually be served out of this cache memory and a few cycles

我们请求的大多数数据实际上都会从这个缓存内存中提供，这只需要花费几个「时钟周期」

Rather than from this slow main memory

而不是从这个缓慢的主存

Now cache memories are managed completely in hardware

高速缓存存储器完全由硬件管理

So this means that the heart there's ,here has to be hardware logic that knows

所以这里关键是，硬件逻辑得知道

How to look for blocks in the cache and determine whether or not a particular block is contained there

如何查找缓存中的块，并确定是否包含特定块

So cache memories are have to be organized in a very kind of strict simple way

因此，必须以非常严格且简单的方式去组织高速缓存存储器

So that the logic the lookup logic can be pretty simple

查找逻辑可以非常简单

So this is very all cache memories are organized in the following way

所有缓存存储器都按以下方式组织

You can think of the cache as an array of S=2^s sets

你可以将缓存视为由 S=2^s 个「组」构成

Ok each set consists of E=2^e lines

每一组都包含 E=2^e 「行」

Where each line consists of a block of B=2^b bytes of data

其中每一行由一个 B=2^b 字节的数据块组成

A valid bit which indicates whether these data bits are actually that the bits and the data block are actually meaningful right

存在一个「有效位」，指示这些数据位和数据块实际上是存在的

It's possible they could just be random bits like you know when you first turn on the machine

当你第一次打开机器时，它们可能只是随机比特位

There's nothing in the cache

缓存中没有任何实际内容

But those bits will have values right that they'll lead to be ones or zeros

这些位将具有值，要么为 1 要么为 0

But they won't actually correspond to data

但它们实际上并不属于数据的一部分

Okay so the valid bit tells us if these B bytes actually mean anything

有效位告诉我们这些 B 字节是否实际意味着什么

And then there's some additional bits called the tag bits

然后，还有一些称为「标记位」的附加位

Which will help us search for blocks which I'll show you in a minute

这将帮助我们搜寻块，我将马上向你展示

Now when we talk about our cache size

现在我们谈谈缓存大小

We're referring to the number of data bytes that are contained in blocks

我们指的是块中包含的数据字节数

And so each cache has there's S sets

所以每个缓存都有 S 组

There's E blocks per set

每个组都有 E 块

And there's B bytes per block

并且每个块有 B 个字节

Ok so the total cache size C=S*E*B

所以总缓存大小 C=S*E*B

Ok now so there's a lot of terms to sort of keep straight and it's very easy to get

好了，这些名词确实有点难以理清，而且很容易混淆

To confuse the difference between lines and blocks and lines and sets

混淆行和块以及行和组之间的不同

Okay so we'll go through some examples and hopefully these will see these will start to make more sense

好的，我们将通过一些例子，希望这些能够你们帮助你们理解

Now let's look at in general how the cache hardware implements a read

让我们来看一下缓存硬件如何实现读取

So when our program accesses, when our program executes an instruction

当我们的程序访问时，程序会执行指令

That references some word in memory

这引用了主存中的一些字

The CPU sends that address to the cache

CPU 将该地址发送到缓存

And asks and it asks the cache to return the word the word at that address

询问并要求缓存在该地址返回字

So the cache takes that address

因此缓存占用该地址

This would be a 64-bit address in case of x86-64

对于 x86-64，这将是 64 位地址

And it divides the address into a number of regions

它将地址划分为多个区域

Which are determined by the organization of the cache

这由缓存的组织决定

Okay they're determined by those parameters S sets

他们是由那些参数 S 组决定的

The s the number of sets a the number of lines per set and b the size of each data block

s 设置的数量是每组的行数，b 是每个数据块的大小

So the low order bits there are b low order bits which determine the offset in the block

因此，b 个低位地址，用于确定块中的偏移量

That that word starts at

那个字开始

Okay the next s bits are treated as an unsigned integer

接下来的 s 位被视为无符号整型

Which serves as an index into the array of sets

它作为组的集合的索引

Okay remember we just think of these as think of this cache as an array of set

记住，我们只是将这些缓存视为一个组的数组集合

The set index bits provide the index into this array of sets

设置的索引位为这个数组提供索引

And then all of the remaining bits

剩下的所有比特

All of the remaining t bits constitute what we call tag

所有剩余的 t 位构成了我们称之为「标签位」的东西

Which will help us when we do our search

这将有助于我们进行搜索

So the cache logic takes this address

缓存逻辑采用此地址

And it first extracts the the set index

它首先提取集合索引

And uses that to,as an index into this array to identify the set that

并使用它作为此数组的索引来识别该组

If this block is in the set,I'm sorry,if the data word

如果这个块在集合中，抱歉，如果是数据字

If the block that contains the data word at this address is in the cache

如果包含此地址的数据字的块存在缓存中

It's going to be in the set denoted by the the set index

它将在 set 索引表示的集合中

So first it identifies which index to look in

首先，它确定要查看的索引

And then it checks the tag,it checks all of the lines in that set

然后它检查标记位，它检查该组中的所有行

To see if there's any of those lines have a matching tag

要查看这些行中是否有任何匹配的标记

That a tag that matches the the t the tag bits and the address

这是一个与标签位和地址匹配的标签

And it checks to see if the valid bit is turned on

并检查有效位是否置为一

So if those two conditions holds if there's a line anywhere in the set

因此，如果这两个条件成立，那么组中的每个位置都有一行

As of where the valid bit is one and there's a matching tag

至于有效位是 1，并且有匹配标记的位置

Then we have a hit okay then the block that we're looking for is contained in this set

那我们称之为「命中」，我们正在寻找的块包含在这个组中

Okay if we...once we determine that with that we've identified the block

一旦我们已经确定了该块

Then the cache uses that the low-order b bits to determine where that

然后缓存使用低 b 位来确定它的位置

Where the data we're interested in begins okay within that block

我们感兴趣的数据在该块内

All right let's look at a more specific example for a the simplest kind of cache

我们来看一个关于最简单方式的缓存的具体的例子

Which is when E=1 when there's only one line per set

当每组只有一行时 E=1

Okay so E=1 one line per set

所以 E=1 每组一行

This kind of cache is called a direct mapped cache

这种缓存称为「直接映射缓存」

So here we have S sets each set consists of a single line

这里我们有 S 组，每组由一行组成

And now suppose our program references the data item

现在假设我们的程序引用了数据项

And a particular address the CPU sense that address to the cache

并且 CPU 感知到缓存的地址

The cache takes that address breaks it up into these into these three fields

缓存将该地址分解为这三个字段

For this particular address the block offset is four

对于此特定地址，块偏移量为四

And the set index is one and then there's some tag bits which we'll just denote with is a color pink

组索引是 1 ，然后有一些标记位，我们只是用粉红色表示

So the cache extracts the set index which is one

因此缓存提取的组索引为 1

And then it uses that as the index into the set

使用它作为组的索引

And then it just ignores all the other the sets

只是忽略了所有其他的组

If the block we're looking for is...is in the cache it's going to be in this inset number one

如果我们正在寻找的块是...在缓存中它将在这个插入的第一个

Then it does the comparison of the tag bits and the valid bits

然后它进行标记位和有效位的比较

And assume that they assume that valid bits on and that it matches

并假设他们有效位和它匹配

Then it looks at the block offset which is four

然后它查看块偏移量为 4 的地址

And which tells it that the four bit in suppose that that's what the instruction was referencing

并且它告诉它这 4 比特就是指令引用的内容

The four byte in begins that offset for

4 个字节开始偏移为

So now the cache takes this int and it sends it back to the to the CPU which puts it in the register

缓存把这视作 int 格式，将它发送回 CPU，并将其放入寄存器中

Okay if the tag doesn't match then the old-line, if the tag doesn't match then there's a miss

如果标签与旧行不匹配，如果标签位不匹配，则表示「未命中」

And in that case the cache has to fetch the block the corresponding block from memory

在这种情况下，缓存必须从内存中获取相应块

And then overwrite this block in the line

在行中覆盖此块

And then it can serve,then it can fetch,it can get the word out of the block and send it back to the processor

然后它就可以服务，它可以从块中取出字并将其发送回处理器

Okay now let me ask you a question just to see you kind of check to see you're following along with this

现在让我问一个问题，只是为了检查你是否跟上了进度

So if there's a miss

如果有一个未命中

And the cache has two requests the block for memory

并且缓存有两个向内存的请求

Fetch it from memory and then overwrite the the block in the current line

从内存中获取它，然后覆盖当前行中的块

Does it also have to change the tag bits or do those stay the same?

是否还必须更改标记位或保持不变？

So does the do the tag bits that were in this line get overwritten with a different value

那么这一行中的标记位是否会被不同的值覆盖

Or is it the same? same? different? same? different?

还是一样吗？相同？不同？相同？不同？

Now why would it be different ?

现在为什么会有所不同

-we haven't changed yes-[student speaking]-I'm sorry oh-[student speaking]

 - 是的，我们没有改变 -  [学生说话]  - 对不起哦 -  [学生说话]

Oh it almost certainly has different data

哦，几乎可以确定它的数据不同

But just have a different address

但只是有一个不同的地址

Exactly it missed because the tag

因为标记位而发生未命中

It missed because the tag didn't match

因为标记位不匹配，而未命中

If the valid bit was false and the tag match then that would also be a miss

如果有效位为假即使标记位匹配了，那么这也是未命中

Oh then you wouldn't...okay that's right,that's...okay good good good,okay,great

那你就不会......那是......好吧

All right let me do a little, let me do a really simple specific example of a how direct map cache works

让我多说一点，让我讲一个非常简单的具体例子，关于直接映射如何工作的例子

I want you to understand in real detail how this would work

我希望你能真正了解这是如何工作的

But I also want to make a point for about the weakness of direct mapped cache is and why

但我也想说明直接映射缓存的不足及其原因

Why you would want to have more than one line per set

为什么希望每组包含多行

Okay this is a really simple we have our memory system consists of 16 bytes

这是一个非常简单的内存系统，由 16 个字节组成

Ok so it's not a very useful system with 4 bit addresses

一个不是很够用的 4 位地址系统

And it's broken up into blocks of 2 bytes each

它被分解为包含 2 个字节的块

Our cache consists of 4 sets with one block per set

我们的缓存由 4 组组成，每组一个块

Now 4 bytes,4 bit addresses

现在 4 个字节，4 位地址

Because B=2, that's 2^1 we only need 1 block offset bit

因为 B=2，即 2^1 ，我们只需要 1 个块偏移位

Right there's only 2 bytes in a block so the byte we're looking for is either at 0 or 1

在一个块中只有 2 个字节，所以我们要查找的字节是 0 或 1

Okay because we have 4 sets we need set index bits

因为我们有 4 组，我们需要设置索引位

And then the remaining bits are always tag bits in this case there's just one tag bit

最后剩余的位总是标记位，在这种情况下只有一个标记位

All right now let's suppose that our program executes instructions

假设我们的程序开始执行指令

That reference the following memory address is 0,1,7,8 and 0

引用地址为 0,1,7,8 和 0 的内存

And these references are reads that they're reading one byte per read

这些引用它们每次读取一个字节

Okay like I said this is a really simple system

我说了这是一个非常简单的系统

So let's look at what happens now

让我们来看看现在发生了什么

We start tag,initially,cache is empty valid bits are all set to zero

我们开始检查标记位，初始时，缓存是空的，有效位都被设置为零

And now the cache receives the request for the byte that's at address 0

缓存接收对地址为 0 的字节的请求

So it extracts the set index bits which in this case are 00

因此它提取组索引位，在本例中为 00

So these so it's going to look in set 0

它将在第 0 组中查找

For and in this case since valid is 0 it's just a miss ok

在这种情况下，因为有效位是 0，它只是一个未命中

So it fetches that block from memory sticks the block

它从内存中取出该块

So this memory this is the...this is using array notation for memory

这个内存就是......使用数组符号来表示内存

So this is like the the bytes that extend from offset 0 to offset 1 inclusive in memory

这就像在内存中，从偏移 0 延伸到偏移 1 的字节

The tag bit is 0 and the valid bit is 1

标记位为 0，有效位为 1

Ok now the next address that comes by is for address 1

下一个选取的地址是地址 1

Well that's a hit right...because we...that block the block that contains the byte at address 1 is already in the cache

这是一个命中...因为我们...包含地址 1 字节的块已经在缓存中

The tag and the tags match okay so we're good that's a hit

标签位匹配正常，很好，这是一个命中

You now we get address 7

我们得到地址 7

So the cache extracts the set index bits,which in this case are 11 or 4 or 3 rather

缓存提取组索引位，在这种情况下是 11 或 4 或 3

Looks in set 3 there's no valid bit,so that's a miss and it loads the the data from memory

在第 3 组中查找，发现有效位为零，因此这是一个未命中，并且它会从内存加载数据

That spans bytes 6~7

跨越字节 6~7

In this case the the tag bit is 0

在这种情况下，标记位为 0

Okay so we record that in our metadata

我们在元数据中记录了这一点

Okay the next reference that comes by is 8

下一个引用是地址 8

Now 8 has a set index of 0,00

8 的集合索引为 0,00

But that's currently occupied by block zero one

但那个目前被块 0-1 占据

And we can tell that because address eight has a tag of one

我们可以发现，因为地址 8 的标签位为 1

And the existing block,the block at the earlier address at address zero has a tag of zero so that's a miss so

而现有的块，地址为 0 的早期地址处的块具有 0 标记，因此这是一个未命中

So now we have to go fetch the block containing byte number eight into memory

所以我们必须将包含字节数 8 的块提取到内存中

So now we have bytes 8-9 and we in our new tag bit

我们有 8-9 字节，我们设置了新的标签位

Okay now the next instruction is for byte 0

现在下一条指令是字节 0

And we just replaced,we had that it,we had that in our cache and we just replaced it

我们刚刚更换了，也就拥有了这个块，即在缓存中有了它，我们只是替换它

So it's another miss so that's unfortunate

不幸的是，这是另一个未命中

And it's the only reason we missed it is because we've got just one line per set

我们错过它的唯一原因是，我们每组只有一行

Right so we were forced to overwrite

所以我们被迫覆盖它

That that block containing bytes,the block zero one when we missed on block eight nine

那块包含字节的块，当我们在块 8-9 时未命中时的块 0

Okay and you see there's plenty of room in our cache we've still got,we've got two lines

你看我们的缓存中有足够的空间，我们还有两行

That we haven't even access right so we've our cache is plenty big

我们甚至都没有访问权限限制，因此我们的缓存非常大

But just because of the low associativity of our cache

但仅仅因为我们的缓存关联性低

And the the sort of the pattern the access pattern that we were presented with

以及我们呈现的访问模式的类型

We've got a miss that really was kind of unnecessary

我们有一个有点不必要的未命中

So oh yeah sorry

不好意思

[student speaking]

[学生说话]

Six,so when we referenced a seven

6，所以当我们引用 7

It's actually the it's at offset one in that block 6-7

它实际上就是 6-7 区块中的偏移量

Okay since blocks are two bytes they'll always start on an even multiple

因为块是两个字节，它们总是以偶数倍开始

Any other questions

还有其他问题吗

Okay so this sort of is the reason why you have caches have higher associativity,higher values of E

所以这就是为什么缓存需要具有更高的关联性，更高的 E 值的原因

So let's look at...and so for values of E greater...for values of E greater than greater than 1

让我们看看......对于 E 的值大于 1 的情况

We refer to them as E way set associative caches

我们将它们称为 「E 路相连高速缓存」

So here E=2 so it's a 2-way it's 2-way associative

所以这里 E=2 ，所以它是两路组相联缓存

Let's suppose we have a 2-way associative cache

假设我们有一个两路组相联缓存

So here we have an array of sets and now each set contains two lines ok instead of one line

这里我们有一系列的组，现在每组包含两行，而不是一行

And suppose we're presented with an address with the following form

并且假设我们被提供了具有以下形式的地址

We're looking for the word that begins at an off set of four inside our block

我们正在寻找从区块内的偏移量 4 开始的字

At within set number one

在第一组内

Okay so the cache expects tracts that set index

缓存需要提取组索引

So this is set 0,this is set 1,this is set 2,throws away all the other sets

这是组 0 ，这是组 1 ，这是组 2，抛弃其他的组

And now in parallel it searches,it searches the tags,it searches for a matching tag in both of these lines

现在开始并行搜索，搜索标记位，在每组的两行中搜索匹配的标签位

And a valid bit so if we get a matching tag and a valid bit true

如果我们得到匹配的标记位和有效位

Then we've got a hit

就是缓存命中

Now that yes yes

是的

[student speaking]

[学生说话]

Oh it's a very good question,so there's hardware logic that does that compare

哦，这是一个非常好的问题，这里的确存在比较检查的硬件逻辑

And that's the reason that as the number of...as the associativity goes up that logic gets more and more expensive

因为随着关联性的增加，逻辑电路变得越来越昂贵

Okay it's like something...like you're kind of doing some kind of tree search

这就像是某种东西......就像你在做某种二叉树搜索一样

And so that actually is the limit that's why..

所以这实际上是限制的原因

I mean because in general right that if you take this to the limit there's just one set

意思是说，通常情况下，如果你把要求电路特别便宜，那么只能存在一个组

With there's just...we call that a fully associative cache so there's just one set

我们称之为「全相联高速缓存」，因此只有一个组

And now any block...a block can go anywhere

而现在任何一个块可以存在于任何地方

Right there's no constraints now where you place a block

你放置一个块，不存在任何限制

But because of the complexity of that fully associative search

但由于完全关联搜索的复杂性

Those are very rare in fact we do see we'll see fully associative caches but their software caches

实际情况中，它是非常罕见的。我们确实会看到全相联高速缓存，除非是在软件级别的缓存中

Okay so in software,so the complexity the hardware

好的，在软件中，硬件的复杂性

And sort of doesn't

而

Doesn't it's not worth the complexity of the hardware for the penalty of having a lower associativity

对于具有较低关联性的惩罚，不值得硬件的复杂性

Okay but there are some systems later on when we study virtual memory

但是稍后我们研究虚拟内存时，会有一些这样的系统

In a virtual memory system the DRAM serves as a cache for data stored on the disk

在虚拟存储器系统中，DRAM 作为存储在磁盘上的数据的高速缓存

And as we saw last time the penalty for a miss

正如我们上次看到的未命中

If you have a cache on DRAM and you miss and you have to go to disk

如果你在 DRAM 上有一个缓存, 但你未命中，你必须转到磁盘

The penalty is huge for that

这个耗时是非常巨大的

And so because of that it's worth while having very complex search algorithms

因此，拥有比较复杂的搜索算法是值得的

In particular in a virtual memory system that the DRAM is implements a fully associative cache where blocks from disk can go anywhere

特别是在虚拟存储器系统中，DRAM 实现了完全关联的高速缓存，其中来自磁盘的块可以存储于任何地方

We'll get into that later when we look in virtual memory

当我们学习虚拟内存时，会更深入的了解此内容

But you're right you'll see in real systems

无疑，你会在真实的系统中看到它

Nowadays that the number goes up right because feature sizes are going down and

现今，数目在快速增加，因为特征尺寸在减少

Designers can afford to implement more expensive hardware

设计人员可以负担得起更昂贵的硬件

But the largest associativity are Intel systems that I know of is 16-way associative L3 caches

我所知道的最大的关联性是 Intel 系统，是 16 路组相联 L3 三级缓存

And then the others are 8-ways associative

其他的大多是 8 路组相联

So that's sort of the order of magnitude that's state of the art right now

这就是现在最先进的组数大小

Okay so then once we've identified a match we use the set offset bits

一旦我们确定匹配，我们就检查组偏移位

In this case we're accessing a short int, so four is the offset within the block of this

在这种情况下，我们正在访问一个短整型，因此 4 是该区块内的偏移量

The two byte short int which then we can return to the processor

两个字节短整形，然后我们可以把数据返回给处理器

Alright so let's do that same simulation that we did before

让我们做之前做过同样的模拟

But this time with a 2-way associative cache

但这次使用了 2 路组相联

Now memory system is the same

内存系统是一样的

But now instead of one set we have two sets

但现在我们有两个组，而不是一个组

And I mean I'm sorry instead of four sets we have two sets

抱歉，我的意思是说，我们有两套而不是四套

So the cache,this is the same sized cache

所以这是相同大小的缓存

But we're just going to organize it differently, instead of 1-way instead of a direct mapped cache

但我们只是以不同的方式组织它，而不是单路组相联高速缓存，而不是直接映射缓存

With four lines containing four lines,one line per set

包含有四行，每组一行

We're going to implement a 2-way associative cache where we have two sets with two lines per set

我们将实现一个双路组相联高速缓存，其中我们有两组，每组又有两行

Okay so each case there's four total lines

所以案例总共有四行

Question

请讲！

[student speaking]

[学生提问]

Oh so that that comes in with the request somehow

以便以某种方式提出请求

And I actually don't know the details of that

实际上，而我并不知道具体细节

It may...I guess there...it could ask for just...

它可能......我猜...它可以要求......

there could just be a default sighs  maybe it's always a 64-byte word and then the processor extracts that the current bits

这可能是一个，它总是一个 64 位，然后处理器提取当前比特位

I actually don't know the details of that

我实际上不知道那个细节

But it either comes in on the request or there's a standard size that the processor then parses out

但它要么在请求中，要么处理器解析标准大小

We'll just assume that the cache knows the what size to return yes

我们假设缓存知道返回的大小

[student speaking]

[学生说话]

How do you decide which block to replace?that's a really good question

如何决定要替换哪个块？这是一个非常好的问题

So there's a lot of different algorithms

有很多不同的算法

The most common algorithm or a common algorithm is least recently used

最常用的算法是「最近最少使用」策略

So by locality you want to keep blocks in the cache that are being used a lot

根据局部性原则，你希望将缓存中的块将被尽可能多次的使用

And so if a block isn't referenced for a long time by the principle of locality by sort of the inverse locality principle

逆着局部性原则思路来思考，如果一个块长时间不被引用，

It's likely not to be addressed referenced in the near future

在不久的将来，它也不太可能会被引用

So that's one algorithm

这就是一种算法

That you just keep track of and I'm not showing there needs to be additional bits

你只是跟踪，我没有说那里需要额外的位

In the line to sort of keep like sort of virtual timestamps that

类似于在排序中，保持虚拟时间戳

But that's sort of the general way you do it

但这只是你做这件事的常规方式

Just try to keep the things that are the blocks being accessed the most frequently,most recently yes

只是尽量保持最常访问的块

[student speaking]

[学生说话]

Okay the question is what determines the block size

好问题，是什么决定了块的大小

That's determined by the design of the memory system

这是由内存系统的设计决定的

So that's a fixed parameter of the memory system

这是内存系统的固定参数

So when the intel designers decided to put cache memories on their processors

因此，当 Intel 设计师决定将缓存存储器放在他们的处理器上时

They decided that the block size would be 64 bytes

他们决定了块大小为 64 字节

Sorry

不好意思

So the block size comes the block size comes first

因此，先解决块大小

Then you determine how big you want your cache to be

然后决定你所期望的缓存的大小

Okay and you determine the associativity

然后你再确定关联性

And then once you've determined the associativity and you know how big your cache is

一旦确定了关联性，就可以知道缓存有多大

Then that determines the number of sets

然后确定组的数量

Okay so basically all of those the

基本上所有这些

The the number of lines and the cat and the capacity

行数的数量和容量大小

The number of lines per set is sort of a fixed high-level parameter design parameter

每组的行数是固定的高级设计参数

The size of the cache is a high-level design parameter

缓存的大小是高级设计参数

And then the number of sets then is induced from that

由此可以推断组的数量

Okay yes

是的

[student speaking]

[学生说话]

Ah that's yeah how does...so that's the replacement policy

那是怎么如何实现的......这就是替代策略

So the question is how does it when there's multiple lines in a set how does it determine which to over overwrite

如何在一组中有多行时，确定哪些将被覆盖

And that was the previous question probably maybe I should have repeated it

这是之前的问题，也许我应该重复一遍

So you try to pick a line that was least recently used

你应该尝试选择最近最少使用的行

So lines that haven't been accessed

所以最近没有访问过的行

Recently are good candidates for replacement because

很适合替换，因为

Because of the sort of inverse locality principle right that

由于这种逆向局部性原理的正确性

They haven't been inverse referenced recently chances are they won't be referenced

它们最近没有被引用，它们将来不会被引用的可能性也很大

Again it

再一次

Oh yeah there's additional bits that I'm not showing here that you have to

哦，是的，这里还有一些东西我还没提到

So when you replace a line in the set

当你替换组中的一行时

If that data is has changed then it has to be written back to memory

如果该数据已更改，则必须将其写回内存

And that's another bit I haven't shown

这是我没讲的地方

Yes

请讲

[student speaking]

[学生说话]

Ah so yeah,so this is a really this is really tricky parameter writing

啊，所以是的，所以这真是一个非常棘手的参数问题

It's a high level system parameter that it goes on for years

它是一个高级系统参数，它持续多年

So the idea you want to have blocks in order to exploit spatial locality

你想要通过块以利用空间局部性的想法

Right think about if you're going to go to the trouble of if you have a miss in cache

想一想，如果你缓存未命中，你是否会遇到麻烦

And you're going to go to the trouble of going all the way to memory to get some data

而且你将会遇到麻烦，只能去内存中获取一些数据

You want to amortize the cost of fetching that data by fetching more than one byte

你希望通过获取多个字节，来分摊获取该数据的成本

That's the motivation for blocks

这是块的本意

Because by the principle of locality in spatial locality in particular

因为特别是在空间局部性的原则上

If you reference a word inside of a block

如果你引用块内的字

Chances are you're going to reference a nearby word which will also be an epilogue

你可能会引用附近的字，这也是一个结尾

Okay so blocks the whole purpose of blocks is to exploit spatial locality

块的整个目的就是利用空间局部性

Now if you make your block too small then you don't amortize

如果你的块太小，那么你就不会摊销

You don't get the same amortization right you maybe get one

你没有获得相同的摊销，你可能得到一个

You bring the block in

你把块带进来

So there's a reference you get a miss you bring the block in

有一个引用，你得到一个未命中，你带来了块并放置于此

There's another reference nearby you get a hit because the blocks in memory

附近有另一个引用，因为内存中已存在的块，你会命中缓存

But then the next reference is in a different block

但接下来的引用是在不同的块中

Because your block sizes are too small

因为你的块实在太小

Right so you kind of want to make blocks big as big as possible

所以你想要使得块尽可能大

But without slowing the system down

但不至于减慢系统速度

So if you made your block size too big it would just take too long to bring that block in

因为如果你将块大小设置得过大，则需要花费太长时间才能放置该块

Plus now your blocks,that your blocks are taking up bits in your cache memory

再加上你的块占用缓存内存中的位

So now there's no room for other blocks

没有其他块的空间了

Right so it's a really tricky design problem right and

这是一个非常棘手的设计问题

If we're doing it taking an architecture class then we would sort of dive into the

如果我们正在做一个计算机体系结构课程，那么我们就会深入研究

You know how how architects make those design decisions

你将会学习架构师如何做出这些设计决策

But in general that's what it's kind of a balancing act right

但总的来说，这是一种平衡行为

Were there any other questions? yes

还有其他问题吗？

[student speaking]

[学生说话]

Oh the question is every time there's a miss

哦，问题是每次都有缓存未命中

Do you have to select a victim line and override it

你是否必须选择受害者行并覆盖它

Yeah I don't know of any caches that don't do that

是的，我不知道任何不这样做的缓存

Now we'll see when we look at rights

现在我们将看到我们何时看待 rights

We'll see there's an option of whether we're only looking at reads right now

我们会看到有一个选项，我们是否只是立即查看

But with rights that question does come up

但是 rights 确实会出现问题

If you wait in a couple of slides we'll go over that

如果你等待几张幻灯片，我们就会讲到这

Any other questions

还有其他问题吗

Okay so let's look at this two-way associative cache now

好的，现在让我们来看看这个二路组相联缓存

There's one block offset bit

有一个块偏移位

We only have two sets so that we only need one set index

我们只有两个组，所以我们只需要一个组索引

And then the remaining two bits are tagged

然后标记剩下的两个比特位

So let's go through our trace so address zero has a set is in set zero right here that's a miss

让我们进入我们的跟踪，地址零有一个组，在这里设置为零，这是一个未命中

so we load that into memory

我们把它加载到内存中

With the reference to address one

参考地址 1

That's in set zero and that's a hit because that byte is in a block

这是设置为 0 ，这是一个命中，因为该字节在一个块中

The reference to seven is a miss that's in set one so we load that

对 7 的引用是在第一组中的未命中，因此我们加载它

And we were just picking randomly pick one of these two over right

而我们只是随意挑选这两个中的一个

Because the cache is empty

因为缓存是空的

The next reference is to address number eight which is in set zero

下一个引用是地址为零的地址

Now here's the difference between the direct mapped cache and this 2-way set associative cache

这是直接映射缓存和这个双路组相联缓存之间的区别

When we reference address 8

当我们引用地址 8 时

That block has to the corresponding block has to go into set 0

该块以及相邻的块也必须放进组 0

Because of this zero set index bit

因为这个 0 组索引位

But we've got room now because we are set to have room for two lines instead of one

但是我们现在已经有了空间，因为我们可以设置两个组而不是一个组

So when we load that in well if we have an available empty slot

如果我们有一个可用的空槽，那么当我们将其加载好时

We'll put it there we won't overwrite anything right so if possible always try to overwrite empty lines

我们将它放在那里，我们不会覆盖任何正确的，所以如果可能的话总是试图覆盖空行

So now we've got in this set...we've got block 0-1 and block 8-9

现在我们已经进入了这个组......我们有 0-1 区块和 8-9 区块

So when we get our reference to address zero

所以当我们得到我们的地址 0 的引用时

Whereas before with the when we had a conflict miss in the direct map cache

而在之前，我们在直接映射缓存中发生了冲突未命中

Now we can satisfy that that request

现在我们可以满足那个请求

It hits in memory and the cache can satisfy it from the cache instead of going to memory

它在内存中命中，缓存可以从缓存中满足它，而不是去内存取回数据

Okay so that makes sense

明白了吗

Okay now what about writes?

好的，写又是如何呢？

So there's multiple copies of the data

有多个数据副本

Right we're sub setting as we move up the hierarchy we're creating subsets of the data in the caches

我们正在进行子设置，因为我们在层次结构中向上移动，我们在缓存中创建数据的子集

So what we do a write to a word within a block that's currently in the cache

那么我们对当前在缓存中的块内的数据字进行写操作

Okay we have two options

我们有两个选择

We can write that block immediately to memory right

我们可以立即将该块写入内存

We're got a block that's like this big and we're updating a little chunk of it

我们有一个这么大的块，我们正在更新它的一小部分

So we can either do the update and then flush it to memory immediately

我们可以进行更新，然后立即将其刷新到内存中

So that memory always mirrors the contents of memory always mirror the contents of the cache

因此，内存始终保持着缓存的镜像

Okay but that's expensive write

好吧，但这是耗费高的写方式

I mean you know memory accesses are expensive

意思是，你知道内存访问是非常费时的

The other so the other option is what what's called write back

另一个选择就是所谓的「写回」

So in this case when we write to a block in the cache

在这种情况下我们写入缓存中的块

We don't flush it to memory

我们不会将其刷新到内存中

Until we elect that particular line as a victim that's going to be overwritten

直到我们选择的那条特定的组作为被替换者，才会被覆盖

And only then when we're just we sort of defer

只有这样，我们只是推迟写入的时间

The writing to memory until the last possible minute

尽可能的直到最后一分钟才写入内存

We defer it until just before the cache would overwrite that that data block

我们将其推迟到缓存覆盖该数据块之前

Okay so that's called write back

是的，这就叫「写回」

And for write back you need to have some an extra bit in the line that indicates whether that blocks been written to

对于写回，你需要在行中加一些额外的位来指示是否写入了这些块

So the algorithm is when the cache identifies a particular line to overwrite

因此，算法就是：当缓存识别出一个特定行将覆盖时

It checks the dirty bit on that line if it's set then it writes that data to back to disk

它检查该行上的「修改位」，如果已设置，则将该数据写回磁盘

Okay if the data hasn't...if that block hasn't been written there's no point there's no need to write it back

如果数据没有写入，那么没有必要将其写回来

Because it has the same value as the copy of the block on disk

因为它与磁盘上块的副本具有相同的值

Okay now so what about...so that's a write here now what happens if we have a right miss

这里有一个写操作，如果我们有一个正确的缓存未命中，看看会发生什么

So we're doing a write to memory

我们正在对内存进行一个写操作

And the word that we're writing is not contained in any block that's in our cache

我们写的字，不包含在缓存中的任何块中

So we have two options we can do what's called write allocate so we can treat it if there's a miss

我们有两个选项，我们可以叫做「写分配」。如果有一个缓存未命中，那么就能处理了

We can do sort of the symmetric thing that we did with a hit which was create a new

如果缓存命中了，我们可以做一些对称的事情

A newline possibly overwriting an existing line

一个新行可能会覆盖现有行

And then write in so we could create that cache enter that cache line

然后写入，以便我们可以创建该缓存，进入该缓存行

Fetch it from memory and then do the writes

从内存中获取它，然后执行写操作

Okay so this is sort of symmetric to reads writes then

这对于那时的读取来说是对称的

So every write if it misses when the write finishes the that block will be in the cache

因此每一次写未命中，那个块将会在缓存中

And if we do a subsequent read we get a hit

如果我们进行后续读操作，就会有缓存命中

Okay so that's the reason you might want to do that

这就是要这样做的原因

The the other option is just to

另一种选择就是

Don't allocate an entry in the cache don't allocate a new line just write write the data directly to memory

「非写分配」，不要分配新行，只是将数据直接写入内存

You don't really need to understand the distinction between these two things

你真的不需要理解这两件事之间的区别

Different caches use different policies

不同的缓存使用不同的策略

For your own mental model a good model to use is just to assume write back write allocate

对于你自己的心中好的模型，就是假设写回写分配

So just assume that we won't copy the data to disk

假设我们不会将数据复制到磁盘

If there's a hit we won't write it back to disk until the last possible minute

如果发生了缓存命中，我们直到最后时刻才将其写回磁盘

And every time there's a write miss what will create a new entry in the cache

每次有「写未命中」，都会在缓存中创建一个新条目

Okay so that's...that I think that's sort of the simplest model that

我觉得这是最简单的模型

And it's a reason it's a reasonable model that you can use regardless of the particular cache implementation

这是一个合理的模型，无论特定的缓存实现如何，你都可以使用它

Now in a real system so far we've only looked at we've only assumed that there's a single cache

到目前为止，我们假设在一个真实的系统中，只有一个缓存

But in a in real systems there's multiple caches

但在实际系统中，会存在多个缓存

So modern core i7 has well architecture from intel

现代核心 i7 拥有来自英特尔的良好架构

Contains multiple processor cores

包含多个处理器核心

So 4 is a typical number for like desktop systems

所以 4 是类似桌面系统的典型数字

8~12 is typical for server class systems

8~12 是服务器类系统的典型代表

These processor cores can each execute their own independent instruction stream in parallel

这些处理器内核可以各自并行执行它们自己的独立指令流

And each processor core can contains general-purpose registers which that's level 0 in the cache

并且每个处理器内核可以包含通用寄存器，其在高速缓存层次结构中为 0 级

And then two different kinds of L1 caches

然后是两种不同的 L1 缓存

The data cache the L1 d-cache

数据缓存「L1 d-cache」

And the i-cache is the...which is the instruction cache

而「i-cache」是指令缓存

And these are fairly small 32 k bytes they're eight-way associative

这些是相当小的 32k 字节，它们是八路组关联的

And they can be accessed in a very small number of cycles

并且可以在极少的时钟周期内访问它们

The next level of the hierarchy is L2 cache

层次结构的下一级是「L2 缓存」

Which is still fairly small 256 k bytes same associativity

这仍然是相当小的 256k 字节，相同的关联性

And it has a slightly longer access time

它的访问时间稍长一点

And it's unified in the sense that the L2 cache contains both data and instructions

在 L2 缓存包含数据和指令的意义上，它是统一的

Ok so that's all within a single core on the chip

这一切都在芯片的单核心内

And then also on the chip but external to all the cores and shared by all the cores

然后在芯片上，但在所有内核外部，并由所有内核共享的

Is at L3 unified cache which is 8 megabytes and 16 way associative

是「L3 统一缓存」，其大小 8 兆字节和 16 路组相联

With an access time that's like 40 to 75 cycles

访问时间大约为 40 到 75 个周期

So if there's a miss in L1,then the L1 sense or tries to sends a request to L2 to try to find the data in L2

如果在 L1 中出现未命中，则 L1 感知到，然后尝试向 L2 发送请求以尝试在 L2 中查找数据

Since L2 is a little bigger maybe maybe the data hasn't been flushed out of L2 yet

由于 L2 稍微大一点，又或许数据还没有从 L2 中刷新

If L2 can't find it,it sends a request to L3 to see if they can find the data in L3

如果 L2 无法找到它，它会向 L3 发送请求，以查看它们是否可以在 L3 中找到数据

If L3 can't find it then it gives up and it goes off chip to memory

如果 L3 中也找不到它，那么它就会放弃，它会从芯片到内存中消失

Yes question

请说你的问题

Yes name memory is this that's it's the DRAM built of DRAM chips

是的，是内存，这是由 DRAM 芯片构成的 DRAM

It's separate,it's in a separate separate set of chips on the motherboard

它是独立的，存在主板上的一组独立芯片中

Connected by those that I/O bridge

通过那些 I/O 桥连接

That we and the bus various buses then that we talked about last time

我们上次谈到了各种总线

And for all different for all of these different caches that block size is 64 bytes

对于所有这些不同的缓存，所有不同的块大小为 64 字节

Now there's a number of different ways to I think about the performance of caches

现在有很多不同的方法来考虑缓存的性能

My most common way is using a metric called the miss rate

最常见的方法是使用称为「未命中率」的指标

So what this is the fraction of references that miss

这是关于未命中的参考文献的一部分

So we're very so I thought and it's 1- hit rate

这是 1 的命中率

So typical for caches to work that miss rate has to be pretty low

对于正常工作的缓存来说，典型的未命中率必须非常低

And fortunately because of locality these miss rates are low

幸运的是，由于局部性原则，这些未命中率很低

Another metric is the hit time

另一个指标是「命中时间」

So if we do have a hit in the cache how long does it actually take to...

如果我们确实在缓存中有一个命中，它实际需要多长时间...

Sort of look up the you know do the lookup to determine that there was a hit and then return the value

用你所知道的查找排序，确定有一个命中，然后返回值

So for L1 and in an intel system this is 4 clock cycles,10 clock cycles for L2

对于 L1 和在 intel 系统中，这会花费 4 个时钟周期，L2 为 10 个时钟周期

And then there's an additional cost if

还会有额外的费用

So you always have to pay the hit time right the hit time is the best you can do

你总是要付出命中时间，你应该在这部分尽力做到最好

But if you have a miss then it's you pay the hit time

但是，如果你有一个未命中，那么你需要花费命中时间

Because you have to do the search and eventually you're going to have to return the word back to the requester

因为你必须进行搜索，最终你必须将该字返回给请求者

But you then you have this additional cost

你会有这个额外的损失

Which you have to go which is going to the the memory ready to fetch the data

你必须去哪个内存准备好获取数据

Okay so that miss penalty,that's what called miss penalty

这样的未命中所带来的惩罚，这就是所谓的「未命中处罚」

Is on the order of hundreds of cycles for main memory

对于主存储器来说，是大约数百个时钟周期

But at other levels of the hierarchy it can be huge

但在层次结构的其他层面，它可能就会特别耗时

So the miss penalty if you have a cache in main memory

因此，如果你在主内存中有缓存

That's caching blocks that are stored on disk, the miss penalty is enormous

这是存储在磁盘上的缓存块，未命中处罚将会十分严重

So it's kind of interesting if you think about it

如果你想一想，这里有意思的是

The performance of these systems is very sensitive to the miss rate much more sensitive than you would think

这些系统的性能对未命中率非常敏感，比你想象的要灵敏得多

And in fact 99% hit rate is twice as good as a 97% hit rate

事实上，99％的命中率是97％命中率的两倍

Yes

是

[student speaking]

[学生说话]

Yeah they hit,so the question is does the hit time include the time to access the tag and yes

是的，他们命中了，问题是命中时间是否包括访问标记位的时间

So the hit time is the time it takes to just to search

命中时间是搜索所需的时间

To determine if that item is in the cache and then return it

确定该项是否在缓存中，然后将其返回

[student speaking]

[学生说话]

Yeah so the yeah so the the miss the miss penalty is the time it takes for the cache to fetch the data from memory

是的，未命中处罚就是它从内存取回数据所花的时间

So that's all the latency you know going across the buses

它是你所知道的所有延迟的之和

The time it takes the memory to respond to the requests

内存响应请求所花费的时间

The time it takes the data to flow back over the buses back to the the cache

数据通过总线返回缓存所需的时间

So the time for a miss is going to be the hit time plus the miss penalty that clear

未命中的时间将是命中时间加上明确的未命中处罚

So I mean imagine suppose there's a hit time of one cycle and a miss penalty of 100 cycles that those are reasonable numbers

假设有一个时钟周期的命中时间，和100个周期的未命中处罚，这实际上是合理的数字

So the the average access time if you have 97% hits

如果你有 97％ 的命中率，

It's the hit time plus the percentage of misses times the miss penalty

平均访问时间是命中时间加上未命中处罚乘以百分系数

So that's four cycles for the average access time

所以平均访问时间是四个周期

But if we just increase the hit rate by two percent

但如果我们只将命中率提高 2％

The average access time drops by 50% a factor of two

平均访问时间就会减少了 50％，减少了两倍

All right so why is this stuff important why should you care about it

为什么这个东西很重要，为什么要关心它呢？

So cache is that as we've seen are these these they're automatic they're all built in hardware

缓存就像我们所看到的那样，都是自动执行的，是由硬件构建的

There's no part of the sort of the visible instruction set that

不存在那种所谓的可见指令集

Lets you manipulate caches and your assembly machine code programs

去允许你操作缓存和组装机器代码程序

So that it all happens behind the scenes automatically in hardware

这一切都在硬件中自动在幕后执行

But if you know how kit...if you know about the existence of caches

如果你知道缓存的存在

And you have this general idea of how you can work how they work

而且你对它们的工作方式有了一个大概的了解

Then you can write code that's cache friendly

然后你就可以编写缓存友好的代码

In the sense that your code will have a higher higher miss rate than code that that isn't cache friendly

从某种意义上说，你的代码会比不缓存友好的代码具有更高的未命中率

So the idea is to...you want to focus on making the common case go fast

关键在于......你应该专注于使得更常用的部分更加快一点

Don't spend your time on code  that sort of code that doesn't get execute very much

不要把时间花在那些代码不能很快执行的代码上

So look at the most commonly called functions

所以看看最常见的函数

And then within those functions look at the inner loops of those functions

然后在这些函数中查看这些函数的内部循环

Because it's the inner loops that are executing the most

因为它是执行最多的内循环

Right so you can as a first approximation you can just ignore sort of stuff

所以你可以做一个初步近似，忽略一些东西

If you have nested loops you can ignore stuff that's going on in the outer loops

如果你有嵌套循环，你可以忽略外循环中发生的事情

And just focus on the code in the inner loop

然后只关注内循环中的代码

Now what you want to do is try to minimize the misses in the inner loop

现在你想要做的是尽量减少内循环中的未命中

Okay so repeated references to a variable is variables are good

如此重复引用变量是好的

Especially if those are local variables

特别是对于那些局部变量来说

Right so remember if you declare a local variable and see

请记住，如果你声明一个局部变量

The compiler can put that in a register

编译器可能将它放在寄存器中

Right if you're referencing global variables

如果你正在引用全局变量

Maybe not,the compiler doesn't know what's going on

编译器不知道发生了什么

So it can't put the reference to that variable to see in a register

因此它无法将该变量的引用放入寄存器中

Okay so repeated references to local variables stored on the stack are good

如此重复引用存储在堆栈中的局部变量是好的

Because those will get turned into register accesses

因为那些将变成寄存器访问

You'll never go to memory

你永远不会去内存取回数据

Okay also stride-1 accesses two arrays are good

逐元素的访问数组是有利的

And they're good because of the existence of these blocks

由于块的存在，它们是有利的

Right so the only way you'd know that stride one references our good is if you knew that

所以如果你知道的话，唯一的方法就是知道那个步长为一引用的好处

Caches have the 64-byte blocks

高速缓存的块大小为 64 比特

Okay so the and stride-1 reference will have half the miss rate as a stride-2 reference

所以逐步引用相对于步长为 2 的引用，只会有一半的未命中率

Because if you're doing stride-1 references the first reference to a word and a block will miss

因为如果你正在做步长为 1 的引用，则引用对数据字的第一次引用，将会未命中

But then subsequent references will hit

但对其随后的元素的引用将会命中缓存

Right and you'll hit if you're doing a stride-1 reference you're going to hit every word in that block

如果你在做一个步长为 1 的引用，那么将会命中将要访问的块中其余的每个字

If your drive if you're doing stride-2 references you're only going to hit every other word

如果你的驱动器，如果你在做步长为 2 的引用，你只会命中剩下的字

Right so you'll only get,you'll get sort of half the so you'll missed at twice the rate

对，所以你会得到一半的命中率，也就是加倍的未命中率

So

所以

So basically the point I want to make to you is that our understanding of caches

基本上我想对你们说的是，我们对缓存的理解

Allow us to sort of quantify this qualitative notion of locality that we developed the last time

让我们来量化上一次介绍的局部性概念

Right the last time we looked at we said if it's doing stride-1 references that's good if

上一次看到的时候，我们说过如果它正在做步长为 1 的引用，那是缓存友好的

If we're doing if we're accessing the same variable over and over that's good

如果我们一遍又一遍地访问同一个变量，那也是缓存友好的

But if we understand caches now we can quantify it in terms of miss rate

如果我们了解了缓存，我们可以根据未命中率对其进行量化

All right so let's finish up the rest of the class

让我们继续完成课程的其余部分

We're going to look at the performance impact of caches on your code

我们将研究缓存对代码的性能影响

Okay and why you need to why you need to know about these things

为什么你需要知道这些事情

And that the impact that they can have

以及他们会造成的影响

So there's a very interesting function

有一个非常有趣的函数

This actually plotted on the cover of your text book

实际上绘制在教科书的封面上

That we call the memory mountain

我们称之为「存储器山」

I learned about this from a graduate student here at Carnegie Mellon back in the 90s who developed this notion named Tom Stricker

我从 90 年代卡内基梅隆大学的名叫汤姆史翠克的研究生那里了解到这个，他提出了这个概念，

And what it's a the memory mountain plots a measure called read throughput or read bandwidth

存储器山，它描绘了一种称为「吞吐量」或「读带宽」的衡量标准

Which is the number of bytes read from memory

也就是从内存中读取的字节数

So if you have a loop and you're scanning over a vector

如果你有一个循环，而你正在扫描一个数组

So you have a vector of say double words

一个双精度 double 类型的数组

And you're reading those elements from a vector one after the other

而且你正在逐个从数组中读取这些元素

The read throughput is the number of megabytes per second that you can perform that task

读吞吐量是执行该任务的每秒兆字节数

At and the memory mountain plots read throughput

在存储器山图上表示为读取吞吐量

As a function of the temporal and spatial locality in that loop

作为该循环中的时间和空间局部性的函数

Okay so in a sense it's looking at a wide range of locality options or characteristics in a program

从某种意义上说，它考虑程序中的各种局部性选项或特征

And it's plotting the performance of that memory system on that across that range as a two-dimensional function

将该存储系统的性能在该范围内作为二维函数绘制出来

So in some ways the memory mountain is a

从某些角度来说，存储器山就像指纹一样

Kind of a fingerprint right every system has its own unique memory mountain

每种系统都有自己独特的记忆山

That we can measure right by writing a simple program

我们可以通过编写一个简单的程序来衡量

And so the idea here is that to construct the memory mountain

这里是要构建记忆山

We write a program called test

我们编写了一个名为 test 的程序

Oh shoot

哦

For some reason it's not

出于某种原因，事实并非如此

Okay right

好的，没错

So when we build a memory mountain

当我们建立一座记忆山

We're given a vector that consists of a collection of double words

给出了一个由双字组成的向量

And then we write a loop that reads those words that read some number of words in this case

然后我们编写一个循环来读取在这种情况下读取一些字

Hmm

嗯

There we go

开始了

So it reads it reads elems number of elements right

它读取 elem 元素数量

So we've got each of these double word elements with a stride of stride

我们将通过步长 stride 获取每一个双字

Okay so if we have a stride of one

如果步长为 1

I know that was kind of redundant huh

我知道那有点多余了

So if we have a stride of one

如果步长为 1

Then we'll have our loop wills was sort of looped through

然后我们将循环遍历一遍

And read these elements until we've read elems number of those elements

直到我们读完了 elem 个元素

Okay and then we'll do it again and then that warms up the cache

然后我们再次这样做，这会加热缓存

Then we do it again and do exactly the same thing

我们再做一次同样的事情

So if we're doing this with a stride of two then we would be reading

如果我们以两个步长这样做

We would read this word zero or elem 2 elem 4 and so on

我们会读到这个字 0 或字 2 字 4 等等

Okay so well then what all we're doing we're just for wide range of strides

那么我们所做的一切只是因为步长的不同取值范围

And a wide range of sizes

和各种大小

We're scanning over this vector and just recording how long it takes to do that read

我们正在扫描这个数组，只记录读取所需的时间

And then convert we convert that into megabytes per second

然后我们将其转换为每秒兆字节

And in order to I just wanted to show you this is we don't need,we're not going to go into detail about this but

而且为了突出我想告诉你们的，而这是我们不需要知道的，所以不打算详细讨论这个问题

This is actually how I generated the the cover on the book and

这实际上就是我如何制作出书本的封面

In order to use to exploit the parallelism inside the intel processor

为了利用 intel 处理器内部的并行性

Like you learned about last week there's a lot of parallel functional units

就像你上周学到的那样，有许多并行的功能单元

In order to exploit those I did 4x4 loop unrolling

为了利用那些，我做了 4x4 循环展开

So I'm actually doing sort of four scans in parallel

所以我实际上并行进行了四次扫描

But the general idea is just what I've showed you here

但总的大概就是我在这里向你们展示的内容

And this this 4x4 loop unrolling is just an optimization

这个 4x4 循环展开只是一个优化

But I wanted to show it to you because it actually it's the exact same principles

我想向你展示它，因为它实际上是完全相同的原则

You learned about last week professor Bryant talked about code optimization

你上周了解了布莱恩特教授谈到的代码优化

So what we do is,we call this test function with these various ranges of elems and stride

我们所做的是，我们称这个测试函数具有不同大小的 elems 和 stride

And then we measure the performance and we get this beautiful picture

我们评测性能，得到了这幅美丽的图

This beautiful function ,to me it's beautiful I don't know does it look beautiful to you

这个美丽的函数，对我来说它是美丽的，我不知道它对你们来说是否依然美丽

So on the z

所以在 z 轴

On the z axis is plotting read throughput in megabytes per second

在z轴上绘制读取吞吐量，以兆字节/秒为单位

Ranging from 2000 megabytes per second up to 16,000 megabytes per second

范围从每秒 2000 兆字节到每秒 16,000 兆字节

This this axis is measuring is stride

这个轴表示的是步长

So going from stride 1 up to stride 12

所以从步幅 1 到步伐 12

And this axis is...so as we as we increase stride we're decreasing the spatial locality

而这个轴是......当我们增加步幅时，我们在减少空间局部性

Alright

好的

And this axis is the size axis so we're going from I think 16k up to 128 megabytes

这个轴是 size 轴，所以我们认为大约是从 16K 到 128M

So this is the number of elements we're going to read each pass through

这是我们每次传递时要读取的元素总数量

So as we as we increase the size

正如我们增加 size 一样

We're sort of decreasing the impact of temporal locality because word

因为字，我们减少时间局部性的影响

As we increase the size there's fewer and fewer caches in our hierarchy can hold all that data

随着我们增加 size ，我们的层次结构中的可以容纳数据的缓存越来越少，

And so this so we've got spatial locality decreasing in this direction

所以这样我们就可以在这个方向上减少空间局部性

And temporal locality decreasing in this direction

并且时间局部性在这个方向上减少

So as a programmer what you want to do you want to be up here

所以作为程序员你应该想办法在这里，做你认为该做的

Right good spatial locality good temporal locality

良好的空间局部性，良好的时间局部性

Because you can get like 14 gigabytes per second measure agreed throughput

因为你可以达到每秒 14GB 的速度，来衡量一致的吞吐量

You don't want to be down here

你不想在这里

Which is only about 100 megabytes per second where you're reading out of memory

在你读取内存时，每秒只有大约 100 MB

Right so the difference between reading all of your data from memory

从内存中读取数据

And reading it from some part of the the caches is huge it's enormous

和从缓存的某些部分读取它,这两者之间的区别是巨大的，极大的

Ok so because you're 213 students you'll be up here

你们在座的是 213 课程的学生，你们会处于这个存储山的上方

And all the students that didn't take 213 they'll be down here

没有来 213 上课的学生，他们会在这底下

And I've actually had I've actually had people several people write back

有几个同学写信回来告诉我

To tell me about their experiences you know in internships and jobs after they left CMU

告诉我，他们离开 CMU 之后在实习和工作中所了解的经历

Where they were given some code that that was down here

他们在那里被分配负责一些代码

And they recognized the locality issues and they got it you know better up here or close at least better

他们意识到了局部性问题，他们得到了处于上方的缓存友好的代码

So this picture this so-called memory mountain has all kinds of interesting features

这张所谓的存储器山，有各种有趣的特征

First of all there's these what I call ridges of temporal locality

首先，我称之为「时间局部性的山坡」

Where these ridges see these ridge lines if you think of this is like a mountain

如果你认为这就像一座山，这些山脊会看到这些山脊线

You see this ridge line and you see this ridge line

你看到这条山脊线，你看到这条山脊线

And here's another ridge line and then here's a here's another one

而这里是另一条山脊线，然后这里是另一条山脊线

These correspond to different levels in the hierarchy

这些对应于层次结构中的不同级别

So this this top ridge line is where you're reading directly out of L1

所以这条顶部山脊线就是你从 L1 直接读取的地方

And it should be perfectly flat and

它应该是完全平坦的

It's so fast that we're getting like measurement jitter performance jitter right

这是如此之快，以至于我们正在获得性能抖动

But it's and this little drop off here is a measurement artifact it it should it shouldn't be there

这里的一点点下降，是一个多余的测量时间函数

It should be flat and go all the way to the wall back here

它本应该是平的，一直到这里的墙

And then here this ridge line is where we're accessing L2

然后在这里这条山脊线是我们访问 L2 的地方

This is where we're accessing L3

这是我们访问 L3 的地方

And here's what we're accessing mostly from memory

这是我们主要通过内存访问的内容

So you have these ridges of temporal locality

所以你有这些时间局部性的山坡

And then you have these slopes of decreasing spatial locality

然后你有这些空间局部性减少的斜率

So you see the slope here

所以你看到这里的斜坡

As work so as we're moving from the top of the slope down to the bottom

正如我们正在从斜坡顶部向下移动到底部

We're decreasing our spatial locality so we're getting less benefit for these blocks that we're bringing in

我们正在减少我们的空间局部性，因此我们带来的这些块的增益会减少

So you can see the we're getting less benefit out of the cost

你可以看到我们从额外的消耗中获得的收益越来越少

That we went through of importing of fetching these blocks

我们需要这些块的导入

And once the stride hits the block size

一旦步长达到块大小

Now every reference is hitting a different block

每个引用都会遇到不同的块

And so and then it flattens out then you get you're getting none benefit from spatial locality

然后它变得平坦，然后你不会从空间局部性获得增益

And similarly here is where this this slope is where we're reading from L3

这里的斜率是我们从 L3 读取的地方

And it flattens out always they always flatten out at the block size which is a stride these are double words right

它变得平坦，它们总是在达到块尺寸时变平，就是当 stride 等于双字大小时

So it's stride of eight is 64 bytes

步长为 8 就是 64 字节

So once you exceed a stride of eight then you're no longer

一旦你超过8的步伐，你就不再有增益了

You're missing every time in a different block

你每次都在不同的块

There's this interesting this one puzzled me for a while

有趣的是，这个让我困惑了一段时间

You might be wondering like how come like over here is we increase the size

你可能想知道我们在这里如何增加 size

We can sort of getting the...we're sort of as we increase the size

就像增加 size 一样

We're doing most of our references out of caches that are lower in the cache hierarchy

我们从缓存层次结构中较低的缓存中执行大多数引用

Okay but except when we're doing stride-1 references

除非我们在使用步长为 1 的引用

You can see all the way up to right at the end

你可以一直到最后

Right before it exceeds the size of L3

就在它超过 L3 的大小之前

It's flat

它很平坦

And it's running at the L2 rate

并且它以 L2 速率运行

Alright so here's the L1 rate and then it drops off and then it's running at a constant L2 rate

这里是 L1 率，然后它下降，最后它以恒定的 L2 速率运行

Until the data no longer fits in L3

直到数据不再适合 L3

So I think what's going on here is that the the hardware

所以我认为这里起作用的是硬件

The cache L2 cache hardware is recognizing or maybe it's an L1 but

L2 高速缓存硬件正在识别，或者可能是 L1

Some logic in the cache system is recognizing the stride one reference pattern

但是缓存系统中的硬件逻辑，正在识别步长为 1 的引用

Right because it sees all the addresses

因为它看到了所有的地址

It's recognizing that stride-1 pattern

它识别出为步长为 1 的模式

And then it's aggressively prefetching from L3 into L2

然后它积极地从 L3 预存取到 L2

So that those so it's fetching ahead of time it's anticipating

那些提前取出的东西

It's saying look I've gotten five stride-1 references in a row

它说我已经连续五次引用步长为 1

I'm going to go grab a whole bunch of blocks and load them all up

需要去抓取一大堆块，然后加载它们

Because by the principle of spatial locality those blocks

因为根据空间局部性原则

Those blocks are going to be referenced in the near future

这些块将在不久的将会被引用

So this was really neat and this only happened within the last couple years

所以这真的很整洁，这仅仅发生在过去的几年里

So the intel engineers are always hard at work

英特尔工程师总是在努力改善

And maybe by the time the time we do the next the next edition of the memory mountain

也许到我们下一版本的存储器山的时候

Those systems will recognize stride-2 and you know other stride pattarns two

那些系统将识别步长2

But from this data it appears that it's only recognizing stride-1

但从这些数据来看，它似乎只是认识到了步长为 1

Ok so you can real...you we can improve the spatial and temporal locality of our programs

我们可以改善我们程序的空间和时间局部性

In several different ways that one way to improve the spatial locality is to rearrange loops

以几种不同的方式，改善空间局部性的一种方法是「重新排列循环」

And I'll use matrix multiplication as an example

我们将使用矩阵乘法作为例子

So here's a sort of a simple matrix multiplication in code

这是代码中的简单矩阵乘法

Where we're multiplying a times b and adding it

我们把 a 乘以 b 并加上它

We're taking what's in of the c[i][j]

我们正在取出 c[i][j] 中的内容

And then to that we're adding the sum the inner product of row i of a and the row j column j of b

然后我们将 a 的第 i 行和 b 的第 j 行 j 的内积相加

Okay and then so we're going through and for each i,j in this matrix c

那么我们将通过这个矩阵 c 中的每个 i，j

We're computing an inner product and then creating that sum

我们正在计算内积，然后创造这个总和

So we can actually turns out there's a lot of different ways to do matrix multiply

我们实际上可以证明有很多不同的方法来进行矩阵乘法运算

And this is we can permute these these loops

我们可以置换这些循环

In any of six different possible permutations

置换成六种不同的可能排列中的任何一种

So this is a permutation where it's i followed by j followed by k

所以这是一个排列，其中 i 跟着 j，k

But five other possibilities are feasible

其他五种可能性同样也是可行的

And so we can actually analyze those those different permutations

实际上，我们可以分析那些不同的排列

And predict which one will have the best performance

并预测哪一个会有最佳表现

Okay so what we'll do is we'll look at the inner loop

我们要做的就是看看内循环

And we'll look at the access pattern of the inner loops

看看内循环的访问模式

And it's in the access pattern on arrays c,a and b

它位于数组 c，a 和 b 的访问模式中

Okay so let's look at the i,j,k implementation that I just showed you

让我们来看看 i，j，k 实现

So as always we focus on the inner loop

我们始终专注于内循环

And if you notice this inner loop is doing a row wise access of column a

如果你注意到这个内部循环正在对列 a 进行行方式访问

And a column wise access,I'm sorry, a row wise access of array a

并且列优先的访问，不对，行优先访问数组 a

And column wise access of row b

并且列 b 地访问

So row wise of a,column wise of b,we don't really care about c

顺便说一下 b 的列，我们并不关心 c

Because it's not in the inner loop okay so just ignore that

因为它不在内循环中，所以只需忽略它

So given our assumption that we can hold in this case we're assuming that

假设在这种情况下

We can hold for of these integer elements in a in one block

我们可以在一个块中保存这些整数元素

So the row wise access which has good spatial locality will miss one every four accesses

因此，具有良好空间局部性的行式访问发生缓存未命中，每四次访问一次未命中

Okay the very first reference will miss and then the next three will hit

第一个引用将错过，然后接下来的三个将被命中

And then the next reference after that will hit a new block

在那之后的下一个引用将命中到一个新的块

Okay so one out of four references to a will miss

四分之一的引用会未命中

But because the access pattern for b is column wise every reference to b will miss

但由于 b 的访问模式是列式的，因此每次对 b 的引用都会丢失

Okay so the average number of misses per loop iteration is 1.25

每次循环迭代的平均未命中数是 1.25

Okay the j,i,k version is exactly the same pattern

好吧，j，i，k 版本是完全相同的模式

K,i,j is a little different here

K，i，j 在这里有点不同

We're doing row wise access of b

我们正在对 b 进行逐行访问

And a row wise access of c, so that's good right

并且明智地访问 c，这样才对

So now we've got stride-1 accesses on both b and c

现在我们在 b 和 c 上都进行了单元素依次访问

And the reference to a is outside of the loop,so we don't care about it

对 a 的引用是在循环之外，所以我们不关心它

So so both b and c will miss one quarter of the time

b 和 c 都是四分之一的未命中率

Okay so the total average number of misses per loop iteration will be 0.5

每次循环迭代的总平均未命中数将为 0.5

That's pretty good and i,k,j has the same similar behavior

这是非常好的，i，k，j 具有相同的相似行为

Now j,k,i is sort of the exact opposite

现在 j，k，i 恰恰相反

j,k,i does column wise access of a

j，k，i 按列进行访问 a

And column-wise access of c so right we know that's a stinker right

对于 c 的列式访问权限，我们知道这是一个很难找到的东西

And we qualitative well you know it's bad and we can compute that it will miss a one time per loop iteration

而且我们定性很好，你知道它很糟糕，我们可以计算出它会错过每循环迭代一次

So that will be two total of two misses per iteration

这样每次迭代总共会有两次失误

And k,j,i has the same bad pattern

和 k，j，i 有相同的坏模式

Okay so if we look at all these permutations

如果我们看看所有这些排列

You can see that i,j,k and j,i,k miss 1.25 have 1.25 misses

你可以看到 i，j，k 和 j，i，k 平均有 1.25 次未命中

k,i,j has 0.5 misses and j,k,i has 2 misses

k，i，j 有 0.5 个未命中，j，k，i 有 2 个未命中

So clearly it looks like k,i,j and its brethren are the best option

很明显，看起来 k，i，j 及其兄弟是最好的选择

The only difference is that k,i,j has this additional store

唯一的区别是 k，i，j 有这个额外的储存

So there might be a question that is that going to create is that going to slow things down

因此，可能存在一个问题，即创造的是减慢速度

Well it turns out in systems in any kind storage systems rights

在任何类型的存储系统权利系统中都证明了这一点

Are much easier to deal with them reads

处理读操作更容易

Can you think about why that might be true

你能想一想为什么会这样吗？

So writes you have a lot more flexibility than you do with reads

写操作比读取操作更为灵活

I mean yes

是的

That's exactly so you can you have options you can do you can write back defer you can defer writing

这就是你可以做的选择，你可以写回来推迟你可以推迟写

Until the value that you're written is actually used

直到你实际使用的值

But when you read an item you're stuck

但是当你读到一元素时，你就会陷入困境

You can't do anything until you get that data

在获得该数据之前，你无法做任何事情

So it turns out that writes don't really that this additional store doesn't really hurt us

事实证明，写操作并不是， 真的这个额外的存储并没有真正影响到我们

And so when we measure these on a modern system

当我们在现代系统上测量这些时

You can see that that the k,i,j which has the the fewest number of misses

你可以看到 k，i，j 具有最少的未命中数

Has you see we're getting like one miss what we're plotting here is cycles per inter loop iteration

你有没有看到我们，我们在这里绘制的是每个循环迭代的周期

So each iteration is taking about one cycle which is really good

所以每次迭代都需要大约一个周期，这非常好

This i,j,k pattern which is kind of the intermediate 1.2 misses

这种 i，j，k 模式是中间体 1.2 的一种未命中

That's sort of in between and the j,k,i which has two misses per iteration is the worst

这种情况介于两者之间，每次迭代有两次未命中的 j，k，i 是最差的

Ok so what's interesting is we could actually just by doing a little bit of analysis

有趣的是我们实际上可以通过做一些分析

Simple analysis we could actually predict what this graph would look like

可以预测这个图形的样子

Okay in the last ten minutes of the class

在课程的最后十分钟

We're going to look at how to improve temporal locality

我们将研究如何改善时间局部性

Now so what we did with... when we rearranged our loops

当我们重新安排我们的循环时

With in the matrix multiplication what we were doing was in improving our spatial locality right

在矩阵乘法中，我们正在做的是改善我们的空间局部性

But we didn't really do anything to improve the temporal locality

但我们并没有真正做任何改善时间局部性的事情

To improve temporal locality you have to use a technique called blocking

要改善时间局部性，你必须使用称为「阻塞」的技术

And this is important to understand because you're going to need it in your cache lab for one thing

这一点很重要，因为你需要在缓存实验室中完成

But it's also a very general technique

但它也是一种非常通用的技术

Anytime you need,any time you're having issues with temporal locality

任何时候你需要，任何时候你遇到时间局部问题

Okay so

可以，然后呢

We're not going to go into too much detail this code but what I did I rewrote the matrix multiply

我们不打算详细介绍这段代码，但是我重写了矩阵乘法

So that it operates you know a two-dimensional matrix that you can really just think of it as a contiguous array of bytes

它运行时你知道一个二维矩阵，可以把它想象成一个连续的字节数组

So I just rewrote this code to operate on a contiguous array one-dimensional array

我只是重写了这段代码来操作一个连续的数组，一维数组

And then I'm doing the indexing explicitly here

然后我在这里使用显式索引

So here at c[i*n+j] this is n matrix

这里在 c[i * n + j] 这是 n 矩阵

So what I'm doing is...I'm accessing the I'm computing where the I throw starts

我正在做的是......

And then I'm going to the j column of that row and then accessing that element

然后我将转到该行的j列，然后访问该元素

All right so let's...but it's the same idea as before

让我们......但它和以前一样

So let's look at the miss rate for this, this is just an original this is an original unblocked matrix multiplied

因此，让我们看一下这个未命中率，这是一个原始的无阻塞矩阵相乘

So what we're doing is we're computing c[0][0]

我们正在做计算 c[0][0]

And we're doing that by taking an inner product of row 0 and column 0

我们通过采用第 0 行和第 0 列的内积来做到这一点

So if you look at the we're assuming that the cache blocks holds eight doubles

如果你看一下我们假设缓存块有 8 个 double

And that the matrix elements are doubles then we're going to miss one eight of the time

并且矩阵元素是双精度浮点数，那么我们每八次将会有个未命中

Okay so in the first iteration

在第一次迭代中

We're going to miss the first iteration does n of these things

我们将错过这些事情的第一次迭代

And since we're missing n over eight of the time

因为我们在八分之一的时间里都缺席了

We're missing one block for every eight references

我们每八个引用丢失一个块

For each for the first iteration we're going to miss n over eight

对于第一次迭代的每一次，我们将每八次未命中一次

And since there's n for each element for each block I'm sorry

因为每个块的每个元素都有 n，不对

And then oh so this is the number of blocks and the number of misses and then we have n elements

哦，所以这是块数和未命中数，然后我们有n个元素

So that the total number of misses is 9n/8 misses for the first iteration

因此，第一次迭代的未命中总数为 9n/8 次未命中

Okay the second iteration will have the same number of misses

第二次迭代将有相同数量的未命中

Because of our assumptions about the the size of this array

因为我们对这个数组大小的假设

So this these rows are way too big to fit in the cache

所以这些行太大了，不适合缓存

So we never get any we don't get any temporal locality

所以我们永远不会得到任何我们没有得到任何时间局部性

Okay so the total number of misses is 9n/8 times the number of elements that we're updating which is n squared

总失误次数是我们正在更新的元素数量的 9n/8 倍，即 n 平方

Okay so our total misses is 9n/8*n^3

所以我们的总失误是 9n/8*n^3

Now let's rewrite the code to use blocking and so

现在让我们重写代码以使用阻塞等等

You can look at this code later

你可以稍后查看此代码

But it's much simpler just to look at it pictorially

但是以图形方式看待它会简单得多

So what we're doing instead of updating one element at a time

我们正在做的不是一次更新一个元素

We're updating a sub block a b by b sub block

通过 b 子块更新子块 a b

And we're doing that just totally analogously to when an original case where b=1

我们这样做完全类似于原始情况，其中 b = 1

This b by b sub block and c is computed by taking an inner product of the sub blocks

这个 b*b 子块和 c 是通过取子块的内积来计算的

Of a set of sub blocks in an a with a set of sub blocks in b

a 中的一组子块中的一组子块中的 b

And for each one of those we're doing a little mini matrix multiplication

对于每一个我们正在做一点迷你矩阵乘法

So we're taking this sub block times this sub block

所以我们将这个子块占用了这个子块

Plus the second sub block of a times the second sub block of of b

加上 b 的第二个子块的第二个子块

Plus the third sub block of a times the third sub block of b and so on

加上第三个子块，一次是b的第三个子块，依此类推

Okay so we're doing the same inner product  operation

我们正在做同样的内部乘积操作

But instead of scalars we're doing it with these little sub these little tiny matrices

但是我们用这些小小的矩阵来代替标量

Ok all right so let's look at,let's look at what happens to the miss rate when we do this

让我们来看看当我们这样做时未命中率会发生什么

So there's there's n over b blocks in any row or column

在任何行或列中都有 n 个 b 块

And since there's b squared items in each block b*b

并且因为每个块 b*b 中有 b 个平方项

There's B^2/8 misses for each block

每个区块都有 B^2/8 未命中

Okay and so then and then since there's there's n over b blocks in each matrix and there's two matrices

然后因为每个矩阵中有超过 b 个块，并且有两个矩阵

There's 2n/B*B^2/8 misses for this first iteration

第一次迭代有 2n/B*B^2/8 未命中

So that works out to be an nB/4 and

所以这可能是一个 nB/4

And the second iteration has the same miss rate

并且第二次迭代具有相同的未命中率

So the total number of misses is the number of misses for each iteration

未命中的总数是每次迭代的未命中数

Times the number of elements in C that we're updating

计算我们正在更新的 C 中元素的数量

Okay which is (n/B)^2

是 (n/B)^2

So that all works out too it's still in n^3*(1/4B)

这样一切都有效，它仍然在 n^3*(1/4B）

So in our first case with no blocking although that the number of misses is asymptotically the same

在我们的第一个没有阻塞的情况下，虽然未命中的数量是渐近相同的

But there's this pretty, this big difference in the constant factor so for no blocking it's 9/8

但是有这个很好的，恒定因素的这个巨大差异，所以没有阻止它是 9/8

For blocking it's 1/4b we're now we can we can just sort of drive that down

为了阻止它的 1/4b 我们现在我们可以把它降低

By by increasing the block size so this gives us some control

通过增加块大小，这给了我们一些控制

But we still we have...we can't make the block the blocks too big because we need to fit three blocks

但是我们仍然有......我们无法让块块太大，因为我们需要适合三个块

In cache at any one point in time

在任何一个时间点的缓存中

Ok so the reason this is a dramatic difference right

这是一个戏剧性的差异吧

And the reason for this is that by doing the blocking we're sort of exploiting

其原因在于，通过阻止我们就是在利用

Once we load a block into memory we're sort of reusing its items over and over again

一旦我们将一个块加载到内存中，我们就会一遍又一遍地重复使用它

So we're exploiting more temporal locality

所以我们正在利用更多的时间局部性

And matrix multiplication has this into this implicit locality

矩阵乘法将这种情况纳入这个隐含的局部性

Because the computation is order n cubed but the size of the data is n squared

因为计算是 n 阶的，但是数据的大小是 n 的平方

Right so we must be reusing some data items

是的，所以我们必须重用一些数据项

Right the problem with our scalar approach is that we were when we were reusing them they weren't in the cache

我们的标量方法的问题是，我们在重用它们时它们不在缓存中

Ok

好

All right so the point that I wanted to make with you is that

所以我想使你了解的重点是

Cache memories although they're sort of built-in automatic hardware storage devices

高速缓存存储器虽然它们是一种内置的自动硬件存储设备

And you can't really control them

你无法控制它们

If you know about them you can take advantage of your knowledge

如果你了解它们，你可以利用你的知识

And exploit them and make your code run faster

并利用它们并使你的代码运行得更快

Okay and the way you do this is like I said focus on the inner loops

你这样做的方式就像我说的那样专注于内循环

Try to do try to do accesses that a stride one

是尝试做一个跨步的访问

And try to maximize to to maximize spatial locality

并尝试以最大化空间局部性

And try to maximize temporal locality by reusing local variables

并尝试通过重用局部变量来最大化时间局部性

Which can then be put into registers

然后可以将其放入寄存器中

Okay so that's it for today good luck with your attack lab if you haven't finished it and

如果你还没有完成 attack-lab，祝你顺利

Don't forget to get started on cache lab this weekend

不要忘记本周末开始 cache-lab